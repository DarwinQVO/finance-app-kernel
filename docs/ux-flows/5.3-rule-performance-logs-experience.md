# UX Flow: Vertical 5.3 - Rule Performance & Logs Experience

> **Vertical:** 5.3 Rule Performance & Logs
> **Focus:** Parser execution metrics, normalization rule performance, queue monitoring, error tracking
> **Last Updated:** 2025-10-25

---

## Overview

This document describes the complete user experience for the Rule Performance & Logs system, covering performance monitoring dashboards, rule optimization workflows, queue health monitoring, and error analysis across document processing pipelines.

**Key User Personas:**
- **Platform Engineer:** Monitors system health, investigates performance issues, optimizes infrastructure
- **Data Engineer:** Optimizes normalization rules, improves data quality, tunes parser configurations
- **DevOps Engineer:** Responds to queue backlog alerts, handles stuck documents, manages worker scaling
- **SRE Engineer:** Conducts performance reviews, capacity planning, trend analysis, SLA compliance

**Core Workflows:**
1. Identify and Optimize Slow Parser
2. Optimize Slow Normalization Rules
3. Investigate and Resolve Queue Backlog
4. Detect and Respond to Performance Degradation
5. Handle Error Spikes
6. Capacity Planning and Forecasting

---

## Journey 1: Identify Slow Parser (Happy Path)

**Persona:** Platform Engineer (Sarah)
**Goal:** Identify parser breaching SLA (<30s p95) and create optimization ticket
**Context:** Weekly performance review, monitoring parser metrics

### Steps

**1. Open Performance Dashboard**
- Sarah navigates to Observability → Performance Dashboard
- Dashboard loads with 4 panels: Parser Stats, Rule Stats, Queue Health, Error Breakdown
- Time range selector defaulted to "Last 24h"
- Auto-refresh indicator shows "Updated 15s ago"

**2. Review Parser Performance Panel**
- Parser Stats panel shows table:
  - Columns: Parser ID, Executions, Success Rate, Latency p50, Latency p95, Error Rate
  - Sort by "Latency p95" descending (highest first)
- Top row: `bofa_pdf_v2` with p95 = 32.4s (🔴 SLA BREACH, target <30s)
- Visual indicator: Red badge next to latency value
- Trend: Small arrow showing +15% vs 7-day baseline

**3. Drill Down to Parser Details**
- Sarah clicks on `bofa_pdf_v2` row
- Detail panel slides in from right:
  ```
  Parser: bofa_pdf_v2
  =====================================
  Last 24h Performance

  Executions: 1,247
  Success Rate: 98.2% (1,225 success, 22 errors)

  Latency Distribution:
  ├─ p50: 9.8s  ✅
  ├─ p95: 32.4s 🔴 (SLA breach: >30s)
  └─ p99: 45.1s 🔴

  Document Size Distribution:
  ├─ Avg: 2.8 MB
  ├─ p95: 5.2 MB
  └─ Max: 12.4 MB (recent increase)

  Error Breakdown:
  ├─ timeout: 15 (68%)
  ├─ pdf_corrupted: 5 (23%)
  └─ out_of_memory: 2 (9%)
  ```

**4. Analyze Slowest Executions**
- Sarah scrolls to "Slowest 10 Executions" table:
  ```
  Upload ID      | Duration | Pages | Size (MB) | Status
  ----------------------------------------------------------------
  upl_abc123     | 45.1s    | 12    | 12.4      | success
  upl_def456     | 42.8s    | 10    | 10.2      | success
  upl_ghi789     | 40.5s    | 8     | 9.8       | timeout 🔴
  upl_jkl012     | 38.2s    | 9     | 8.5       | success
  ...
  ```
- Pattern observed: Large PDFs (>8 MB) correlate with slow parsing (>35s)
- Timeouts occur on large PDFs when execution exceeds 40s

**5. View Recommendations**
- System shows automated recommendations:
  ```
  🔍 Analysis:
  - 92% of slow executions (>30s) involve PDFs >5 MB
  - Timeouts occur when size >8 MB (current timeout: 40s)
  - Average parse time per MB: 4.2s (baseline: 3.5s)

  💡 Recommendations:
  1. Increase timeout from 40s to 60s for large PDFs (>5 MB)
  2. Investigate memory leaks (avg memory usage up 35%)
  3. Optimize regex patterns (profiler shows 40% time in regex matching)
  4. Enable PDF compression pipeline (reduce file size by 60%)
  ```

**6. Create Optimization Ticket**
- Sarah clicks "Create Ticket" button
- Modal pre-filled with analysis:
  ```
  Title: Optimize bofa_pdf_v2 parser (SLA breach: p95 = 32.4s)

  Description:
  Parser bofa_pdf_v2 is breaching SLA (p95: 32.4s, target: <30s).

  Root Cause:
  - Document size increased 133% (1.2 MB → 2.8 MB avg)
  - Users uploading high-resolution scanned PDFs
  - Memory usage increased 35% (possible memory leak)

  Action Items:
  - [ ] Increase timeout from 40s to 60s (short-term fix)
  - [ ] Implement PDF compression pipeline before parsing
  - [ ] Profile parser for memory leaks
  - [ ] Optimize regex patterns (40% of execution time)

  Priority: High (SLA breach)
  ```
- Sarah assigns to parser team, clicks "Create"

**Outcome:** Sarah successfully identified SLA breach, analyzed root cause (large PDFs), and created actionable ticket with specific recommendations.

---

## Journey 2: Optimize Slow Normalization Rule

**Persona:** Data Engineer (Marcus)
**Goal:** Identify and optimize slow normalization rule
**Context:** Monthly rule performance review, reducing processing latency

### Steps

**1. Open Rule Performance Tab**
- Marcus navigates to Performance Dashboard → Rule Performance tab
- Table displays all normalization rules with metrics:
  ```
  Rule ID                  | Type  | Executions | Avg Time | p95 Time | Match Rate
  --------------------------------------------------------------------------------
  merchant_fuzzy_match_all | fuzzy | 12,456     | 145.2ms  | 280.5ms  | 12%
  merchant_exact_top100    | exact | 98,234     | 0.8ms    | 1.2ms    | 78%
  date_format_iso8601      | regex | 45,678     | 2.1ms    | 3.5ms    | 95%
  amount_currency_convert  | exact | 34,567     | 1.5ms    | 2.8ms    | 100%
  ...
  ```

**2. Sort by Execution Time**
- Marcus clicks "p95 Time" column header to sort descending
- Top row: `merchant_fuzzy_match_all` stands out (280.5ms, 200x slower than exact match)
- Visual indicator: Red badge + "⚠️ SLOW" label

**3. Drill Down to Rule Details**
- Marcus clicks on `merchant_fuzzy_match_all` row
- Detail panel shows:
  ```
  Rule: merchant_fuzzy_match_all
  =====================================
  Type: fuzzy (Levenshtein distance)
  Priority: 50 (mid-priority)
  Match Rate: 12% (only matches 12% of observations)

  Performance:
  ├─ Executions (7d): 87,192
  ├─ Avg Time: 145.2ms
  ├─ p95 Time: 280.5ms 🔴
  └─ p99 Time: 420.8ms 🔴

  Total Time Spent: 10.8 hours (last 7 days)

  Cost Impact:
  - At $0.05/compute-hour: $0.54/day ($197/year)
  - Optimization potential: 80% reduction → save $158/year

  Sample Slow Executions:
  ├─ obs_abc123: 385ms (compared against 15,247 merchants)
  ├─ obs_def456: 342ms (compared against 15,247 merchants)
  └─ obs_ghi789: 298ms (compared against 15,247 merchants)
  ```

**4. Analyze Optimization Opportunities**
- System shows AI-powered recommendations:
  ```
  🔍 Analysis:
  - Rule matches only 12% of observations (low hit rate)
  - Executes for ALL observations as fallback (no short-circuit)
  - Compares against 15,247 merchants (high cardinality)
  - Most matches (80%) are for top 100 merchants

  💡 Optimization Strategy:
  1. Add exact match rules for top 100 merchants (covers 80% of volume)
     - Exact match: 0.8ms (vs fuzzy: 145ms) → 180x faster
     - Reduces fuzzy rule executions by 80%

  2. Use fuzzy match only as fallback (priority 10, runs last)
     - Only executes if exact match fails (20% of observations)
     - Reduces total fuzzy executions from 87K/week to 17K/week

  Expected Impact:
  - p95 Time: 280.5ms → 45ms (84% improvement)
  - Total Time: 10.8 hours/week → 1.7 hours/week (84% reduction)
  - Cost Savings: $158/year
  ```

**5. Implement Optimization**
- Marcus clicks "Generate Exact Match Rules" button
- System analyzes last 30 days of data, identifies top 100 merchants by volume
- Modal shows preview:
  ```
  Generating 100 exact match rules for top merchants:

  1. { pattern: "STARBUCKS", canonical: "Starbucks", volume: 12,456 }
  2. { pattern: "AMAZON", canonical: "Amazon", volume: 9,872 }
  3. { pattern: "WALMART", canonical: "Walmart", volume: 8,234 }
  ...
  100. { pattern: "CVS PHARMACY", canonical: "CVS", volume: 456 }

  Coverage: 78% of observations (68,426 / 87,192)
  Priority: 90 (executes before fuzzy match)
  ```
- Marcus clicks "Deploy Rules"

**6. Adjust Fuzzy Match Priority**
- Marcus edits `merchant_fuzzy_match_all` rule
- Changes priority from 50 to 10 (lower priority, executes as fallback)
- Saves changes

**7. Monitor Performance Impact**
- Marcus returns to Rule Performance tab after 1 hour
- Updated metrics:
  ```
  Rule ID                  | Type  | Executions | Avg Time | p95 Time | Match Rate
  --------------------------------------------------------------------------------
  merchant_exact_top100    | exact | 12,345     | 0.8ms    | 1.2ms    | 78% ✅
  merchant_fuzzy_match_all | fuzzy | 2,789      | 142.8ms  | 45.2ms   | 11%

  Impact:
  - Fuzzy executions reduced 78% (12,456 → 2,789)
  - p95 time reduced 84% (280.5ms → 45.2ms)
  - Total time reduced 85% (10.8 hours → 1.6 hours per week)
  ```

**Outcome:** Marcus successfully optimized rule performance by 84%, reducing processing time from 10.8 hours/week to 1.6 hours/week, saving $158/year in compute costs.

---

## Journey 3: Investigate Queue Backlog (Alert Response)

**Persona:** DevOps Engineer (James)
**Goal:** Resolve queue backlog alert and restore normal processing
**Context:** 3:47 AM alert received, on-call engineer responding

### Steps

**1. Receive Alert**
- PagerDuty notification at 3:47 AM:
  ```
  🚨 CRITICAL: Parse Queue Depth Exceeded Threshold

  Queue: parse_queue
  Status: CRITICAL
  Pending: 847 documents (threshold: 500)
  Stuck: 45 documents (threshold: 20)
  Processing Rate: 18 docs/min (target: 45 docs/min)

  [Acknowledge] [View Dashboard]
  ```

**2. Open Queue Monitor Dashboard**
- James clicks "View Dashboard" link
- Queue Monitor panel loads:
  ```
  ┌────────────────────────────────────────────────────────────┐
  │ Queue: parse_queue                           🔴 CRITICAL   │
  ├────────────────────────────────────────────────────────────┤
  │                                                            │
  │  Queue Depth (Area Chart)                                  │
  │  ┌────────────────────────────────────────────────────┐   │
  │  │ 900│                             ▲                  │   │
  │  │ 800│                          ▲ ││                  │   │
  │  │ 700│                       ▲ ││ ││                  │   │
  │  │ 600│                    ▲ ││ ││ ││                  │   │
  │  │ 500│ ─────────────── ▲ ││ ││ ││ ││  [Pending]      │   │
  │  │ 400│              ▲ ││ ││ ││ ││ ││                  │   │
  │  │ 300│           ▲ ││ ││ ││ ││ ││ ││                  │   │
  │  │ 200│        ▲ ││ ││ ││ ││ ││ ││ ││                  │   │
  │  │ 100│     ▲ ││ ││ ││ ││ ││ ││ ││ ││  [In-Progress]  │   │
  │  │   0├─────┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴──────────────────│   │
  │  │     3:00  3:15  3:30  3:45  4:00  4:15  4:30         │   │
  │  └────────────────────────────────────────────────────┘   │
  │                                                            │
  │  Current Metrics:                                          │
  │  ├─ Pending: 847 🔴 (critical threshold: 500)             │
  │  ├─ In-Progress: 15                                        │
  │  ├─ Stuck: 45 🔴 (critical threshold: 20)                 │
  │  ├─ Processing Rate: 18 docs/min 🔴 (target: 45)          │
  │  └─ Oldest Pending: 30 minutes 🔴                          │
  │                                                            │
  │  [Drill Down to Stuck Docs] [Pause Queue] [Add Workers]   │
  └────────────────────────────────────────────────────────────┘
  ```

**3. Analyze Stuck Documents**
- James clicks "Drill Down to Stuck Docs" button
- Stuck documents table appears:
  ```
  Upload ID    | Parser         | Attempts | Last Error          | Age
  ---------------------------------------------------------------------
  upl_abc123   | chase_csv_v1   | 6        | invalid_format      | 28min
  upl_def456   | chase_csv_v1   | 6        | invalid_format      | 27min
  upl_ghi789   | chase_csv_v1   | 6        | invalid_format      | 26min
  ...
  (45 total, all chase_csv_v1 with "invalid_format" error)
  ```
- Pattern identified: ALL stuck documents from `chase_csv_v1` parser
- Error type: `invalid_format` (same error for all)

**4. Investigate Error Details**
- James clicks on first error "invalid_format"
- Error detail modal shows:
  ```
  Error Type: invalid_format
  Error Message: "CSV missing required column 'Transaction Date'"

  Stack Trace:
  at parseCSV (chase_csv_v1.ts:45)
  at execute (parser.ts:120)
  ...

  Context:
  - Parser: chase_csv_v1
  - Version: v1.3.2
  - Deployed: 3 days ago

  Recent Changes:
  - 2025-10-22: Chase bank changed CSV format (added "Posted Date" column, renamed "Transaction Date" to "Date")

  Likely Root Cause:
  Parser expects old column names, but Chase CSV now uses new format.
  ```

**5. Immediate Action - Pause Queue for Affected Parser**
- James clicks "Pause Queue" dropdown → "Pause for chase_csv_v1 only"
- Confirmation modal:
  ```
  ⚠️  CONFIRM QUEUE PAUSE

  This will pause processing for chase_csv_v1 documents only.
  Other parsers will continue processing normally.

  Affected Documents: 45 stuck + 203 pending = 248 total

  [Cancel] [Confirm Pause]
  ```
- James clicks "Confirm Pause"
- Status updated: "Queue paused for chase_csv_v1 (248 documents held)"

**6. Fix Parser**
- James navigates to Parser Management → chase_csv_v1
- Updates column mapping:
  ```diff
  - expected_columns: ["Transaction Date", "Description", "Amount"]
  + expected_columns: ["Date", "Description", "Amount"]  // New format
  ```
- Adds backward compatibility:
  ```typescript
  // Support both old and new formats
  const dateColumn = headers.includes("Transaction Date")
    ? "Transaction Date"  // Old format
    : "Date";              // New format
  ```
- Deploys new version: v1.3.3

**7. Resume Queue and Monitor**
- James returns to Queue Monitor
- Clicks "Resume Queue" → "Resume chase_csv_v1"
- Processing resumes immediately
- Real-time monitoring:
  ```
  Queue Status: 🟢 HEALTHY (resumed at 04:15)

  Recovery Progress:
  ├─ Pending: 248 → 187 → 124 → 65 → 18 → 0 (cleared in 18 minutes)
  ├─ Stuck: 45 → 0 (all resolved with new parser version)
  └─ Processing Rate: 18 docs/min → 45 docs/min ✅ (back to target)

  Timeline:
  03:47 - Alert triggered (847 pending, 45 stuck)
  03:50 - Acknowledged, investigation started
  03:55 - Root cause identified (Chase format change)
  04:00 - Parser updated and deployed (v1.3.3)
  04:05 - Queue resumed for chase_csv_v1
  04:23 - Backlog cleared, processing normal

  Total Resolution Time: 36 minutes
  ```

**Outcome:** James successfully resolved queue backlog by identifying root cause (Chase CSV format change), updating parser, and resuming processing. Backlog cleared in 36 minutes with zero data loss.

---

## Wireframes

### Wireframe 1: Performance Dashboard (Main View)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│ Performance Dashboard                      [Time Range: 24h ▾] [Auto-refresh ⟳] │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│ ┌───────────────────────────────┬───────────────────────────────────────────┐  │
│ │ 📊 Parser Performance         │ 📐 Rule Performance                       │  │
│ ├───────────────────────────────┼───────────────────────────────────────────┤  │
│ │ Parser ID        | p95 Latency│ Rule ID             | p95 Time | Match % │  │
│ │ ───────────────────────────── │ ─────────────────────────────────────────│  │
│ │ bofa_pdf_v2      | 32.4s 🔴   │ merchant_fuzzy_all  | 280.5ms  | 12%     │  │
│ │ chase_csv_v1     | 8.2s  ✅   │ date_format_iso     | 3.5ms    | 95%     │  │
│ │ wells_ofx_v3     | 5.1s  ✅   │ amount_convert      | 2.8ms    | 100%    │  │
│ │ citi_pdf_v1      | 14.7s ✅   │ category_classify   | 12.4ms   | 88%     │  │
│ │ ... (12 more)                 │ ... (18 more)                            │  │
│ │                               │                                          │  │
│ │ [View All Parsers →]          │ [View All Rules →]                       │  │
│ └───────────────────────────────┴───────────────────────────────────────────┘  │
│                                                                                 │
│ ┌───────────────────────────────┬───────────────────────────────────────────┐  │
│ │ 🚦 Queue Health               │ ⚠️  Error Breakdown                       │  │
│ ├───────────────────────────────┼───────────────────────────────────────────┤  │
│ │ Queue Name    | Status | Depth│ Error Type      | Count | Rate           │  │
│ │ ───────────────────────────── │ ─────────────────────────────────────────│  │
│ │ parse_queue   | 🟢     | 145  │ pdf_corrupted   | 32    | 2.1%           │  │
│ │ normalize_q   | 🟡     | 245  │ timeout         | 15    | 1.0%           │  │
│ │ reconcile_q   | 🔴     | 847  │ invalid_format  | 78    | 5.2% 🔴        │  │
│ │                               │ out_of_memory   | 5     | 0.3%           │  │
│ │ [Monitor Queues →]            │                                          │  │
│ │                               │ [View Error Details →]                   │  │
│ └───────────────────────────────┴───────────────────────────────────────────┘  │
│                                                                                 │
│ Last Updated: 15 seconds ago                                 [Export Report ⬇] │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Wireframe 2: Parser Performance Details (Drill-Down)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│ ← Back to Dashboard          Parser: bofa_pdf_v2                    [Export ⬇]  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│ ⏱️  Performance Metrics (Last 24h)                                              │
│ ┌─────────────────────────────────────────────────────────────────────────────┐ │
│ │ Executions: 1,247          Success Rate: 98.2% (1,225 / 1,247)            │ │
│ │                                                                            │ │
│ │ Latency Distribution:                                                      │ │
│ │ ├─ p50:  9.8s  ✅                                                          │ │
│ │ ├─ p95: 32.4s  🔴 (SLA breach: >30s)                                      │ │
│ │ └─ p99: 45.1s  🔴                                                          │ │
│ │                                                                            │ │
│ │ Trend: +15% vs 7-day baseline                                             │ │
│ └─────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                 │
│ 📊 Latency Histogram                                                            │
│ ┌─────────────────────────────────────────────────────────────────────────────┐ │
│ │ 400│                                                                        │ │
│ │ 350│     ████                                                               │ │
│ │ 300│     ████                                                               │ │
│ │ 250│     ████ ████                                                          │ │
│ │ 200│     ████ ████ ████                                                     │ │
│ │ 150│     ████ ████ ████ ██                                                  │ │
│ │ 100│     ████ ████ ████ ████ ██                                             │ │
│ │  50│ ██  ████ ████ ████ ████ ████ ██  ██                          ██  ██   │ │
│ │   0├───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───────────────────────────┤ │
│ │     0-5  5-10 10-15 15-20 20-25 25-30 30-35 35-40 40-45 45-50 50+ (seconds)│ │
│ └─────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                 │
│ 🔴 Slowest 10 Executions                                                        │
│ ┌─────────────────────────────────────────────────────────────────────────────┐ │
│ │ Upload ID    | Duration | Pages | Size (MB) | Status      | Timestamp      │ │
│ │ ────────────────────────────────────────────────────────────────────────────│ │
│ │ upl_abc123   | 45.1s    | 12    | 12.4      | success     | 10:23:45       │ │
│ │ upl_def456   | 42.8s    | 10    | 10.2      | success     | 11:15:22       │ │
│ │ upl_ghi789   | 40.5s    | 8     | 9.8       | timeout 🔴  | 12:08:10       │ │
│ │ upl_jkl012   | 38.2s    | 9     | 8.5       | success     | 13:42:33       │ │
│ │ ... (6 more)                                                                │ │
│ └─────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                 │
│ 💡 Optimization Recommendations                                                 │
│ ┌─────────────────────────────────────────────────────────────────────────────┐ │
│ │ 🔍 Analysis:                                                                │ │
│ │ • 92% of slow executions (>30s) involve PDFs >5 MB                         │ │
│ │ • Timeouts occur when size >8 MB (current timeout: 40s)                    │ │
│ │ • Average parse time per MB: 4.2s (baseline: 3.5s)                         │ │
│ │                                                                            │ │
│ │ 💡 Recommendations:                                                         │ │
│ │ 1. Increase timeout from 40s to 60s for large PDFs (>5 MB)                │ │
│ │ 2. Investigate memory leaks (avg memory usage up 35%)                      │ │
│ │ 3. Optimize regex patterns (40% time in regex matching)                    │ │
│ │ 4. Enable PDF compression pipeline (reduce size by 60%)                    │ │
│ │                                                                            │ │
│ │ [Create Optimization Ticket]                                               │ │
│ └─────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Wireframe 3: Queue Monitor Panel (Real-Time)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│ Queue Monitor                                              [Refresh: 30s ⟳]     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│ 📊 Queue Depth Over Time                                                        │
│ ┌─────────────────────────────────────────────────────────────────────────────┐ │
│ │ 900│                             ▲▲▲                                        │ │
│ │ 800│                          ▲▲▲│││                                        │ │
│ │ 700│                       ▲▲▲││││││                                        │ │
│ │ 600│                    ▲▲▲│││││││││                                        │ │
│ │ 500│ ─────────────── ▲▲▲││││││││││││  [Pending - Red]                     │ │
│ │ 400│              ▲▲▲│││││││││││││││                                        │ │
│ │ 300│           ▲▲▲││││││││││││││││││                                        │ │
│ │ 200│        ▲▲▲│││││││││││││││││││││                                        │ │
│ │ 100│     ▲▲▲││││││││││││││││││││││││  [In-Progress - Blue]                │ │
│ │   0├─────┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴──────────────────────────────────────┤ │
│ │     3:00  3:15  3:30  3:45  4:00  4:15  4:30  4:45  5:00                   │ │
│ └─────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                 │
│ 🚦 Queue Status Cards                                                           │
│ ┌─────────────────────────┬─────────────────────────┬─────────────────────────┐ │
│ │ parse_queue             │ normalize_queue         │ reconcile_queue         │ │
│ │ Status: 🟢 HEALTHY      │ Status: 🟡 WARNING      │ Status: 🔴 CRITICAL     │ │
│ ├─────────────────────────┼─────────────────────────┼─────────────────────────┤ │
│ │ Pending: 145            │ Pending: 245 🟡         │ Pending: 847 🔴         │ │
│ │ In-Progress: 12         │ In-Progress: 8          │ In-Progress: 15         │ │
│ │ Stuck: 3                │ Stuck: 0                │ Stuck: 45 🔴            │ │
│ │ Rate: 45.2 docs/min ✅  │ Rate: 38.7 docs/min     │ Rate: 18.3 docs/min 🔴  │ │
│ │ Oldest: 42s             │ Oldest: 85s             │ Oldest: 30min 🔴        │ │
│ │                         │                         │                         │ │
│ │ Workers: 5/7 active     │ Workers: 8/10 active    │ Workers: 10/10 active   │ │
│ │ (71% utilization)       │ (80% utilization)       │ (100% utilization) 🔴   │ │
│ │                         │                         │                         │ │
│ │ [View Details]          │ [View Details]          │ [View Details]          │ │
│ └─────────────────────────┴─────────────────────────┴─────────────────────────┘ │
│                                                                                 │
│ ⚠️  Active Alerts                                                               │
│ ┌─────────────────────────────────────────────────────────────────────────────┐ │
│ │ 🔴 CRITICAL: reconcile_queue depth >500 (847 pending)                      │ │
│ │ 🔴 CRITICAL: reconcile_queue stuck documents >20 (45 stuck)                │ │
│ │ 🟡 WARNING: normalize_queue depth >200 (245 pending)                       │ │
│ │ 🟡 WARNING: reconcile_queue oldest document >5min (30 minutes old)         │ │
│ │                                                                            │ │
│ │ [Acknowledge All] [View Alert History]                                     │ │
│ └─────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## Multi-Domain Examples

### Finance: Transaction Processing Dashboard

**Domain:** Fintech payment platform
**Parser:** bank_statement_pdf, chase_csv, wells_ofx
**Rule:** merchant_normalization, transaction_categorization
**Queue:** parse_queue, normalize_queue, reconcile_queue

**Use Case:**
Platform engineer monitors transaction processing pipeline, identifying slow bank statement parser (Wells Fargo OFX v3.1.0 with p95 latency 28.5s). Drill-down reveals large OFX files (>10 MB) causing timeouts. Optimization: Implement chunked processing for large files, reducing p95 to 12.3s (-57%).

**Metrics Dashboard:**
- Parser Performance: `wells_ofx_v3` (1,247 executions, 28.5s p95, 97.8% success)
- Rule Performance: `merchant_normalization` (87,192 executions, 145ms p95, 12% match rate)
- Queue Health: `reconcile_queue` (CRITICAL, 847 pending, 45 stuck, 18 docs/min rate)
- Error Breakdown: `timeout` (32 errors, 2.1% rate), `invalid_format` (78 errors, 5.2% rate)

---

### Healthcare: Claim Processing Dashboard

**Domain:** Healthcare insurance platform
**Parser:** hl7_message_parser, claim_form_ocr, fhir_processor
**Rule:** diagnosis_code_mapper (ICD-9 → ICD-10), patient_matcher
**Queue:** claims_processing_queue, hl7_ingestion_queue

**Use Case:**
SRE engineer conducts weekly performance review, identifying degradation in HL7 message parser (p95 latency increased from 5.2s to 12.8s over 30 days, +146%). Root cause analysis reveals increased message size (avg 450 KB → 1.2 MB due to new lab result attachments). Solution: Implement lazy loading for attachments, reducing p95 to 6.5s.

**Metrics Dashboard:**
- Parser Performance: `hl7_v2_parser` (5,234 executions, 12.8s p95 🔴, 99.2% success)
- Rule Performance: `icd9_to_icd10_mapper` (12,456 executions, 42ms p95, 88% match rate)
- Queue Health: `claims_queue` (WARNING, 345 pending, 0 stuck, 32 docs/min rate)
- Error Breakdown: `invalid_hl7_segment` (12 errors, 0.2% rate), `missing_required_field` (8 errors, 0.1% rate)

---

### Legal: Document Processing Dashboard

**Domain:** Legal tech platform (e-discovery)
**Parser:** contract_ocr, case_document_extractor, pdf_text_parser
**Rule:** document_classifier, entity_extractor (parties, dates, amounts)
**Queue:** ocr_queue, classification_queue

**Use Case:**
DevOps engineer responds to queue backlog alert (ocr_queue depth 1,247, CRITICAL). Investigation reveals OCR parser timing out on scanned legal contracts (avg 450 pages, 200 MB files). Stuck documents all from `contract_ocr_v2` parser with "out_of_memory" errors. Solution: Increase worker memory from 4 GB to 8 GB, implement page-by-page processing.

**Metrics Dashboard:**
- Parser Performance: `contract_ocr_v2` (892 executions, 180.2s p95 🔴, 94.3% success)
- Rule Performance: `document_classifier` (15,678 executions, 28ms p95, 92% confidence)
- Queue Health: `ocr_queue` (CRITICAL, 1,247 pending, 67 stuck, 8 docs/min rate 🔴)
- Error Breakdown: `out_of_memory` (45 errors, 5.0% rate 🔴), `ocr_timeout` (22 errors, 2.5% rate)

---

## Accessibility Considerations

**Keyboard Navigation:**
- Tab order: Dashboard panels → Parser table → Drill-down button → Detail panel
- Arrow keys: Navigate chart data points, table rows
- Enter/Space: Drill down to details, acknowledge alerts, export reports
- Esc: Close detail panels, dismiss modals

**Screen Reader Support:**
- ARIA labels for all charts ("Latency histogram showing distribution from 0-50 seconds")
- Live regions for metric updates ("Parser bofa_pdf_v2 latency p95 updated to 32.4 seconds")
- Descriptive button labels ("Drill down to parser bofa_pdf_v2 details", not "View")
- Table headers announced ("Column: Parser ID", "Column: p95 Latency")

**Color Contrast:**
- Text: 4.5:1 contrast (WCAG AA) - #000000 on #FFFFFF
- UI components: 3:1 contrast - #3B82F6 (blue) on #FFFFFF
- Status badges: Color + icon + text (not color alone)
  - 🟢 Healthy (green + checkmark + "Healthy")
  - 🟡 Warning (yellow + warning icon + "Warning")
  - 🔴 Critical (red + X icon + "Critical")

**Focus Indicators:**
- Visible focus ring: 2px blue outline (#3B82F6)
- High contrast mode: 3px white outline with 1px black border
- Focus never hidden (no `outline: none` without alternative)

**Error Handling:**
- Clear error messages ("Failed to load parser stats. Retry in 30 seconds.")
- Retry buttons with keyboard focus
- Error announcement via live region

---

## Performance Characteristics

**Dashboard Rendering:**
- Initial dashboard load: <1.5s (includes 4 panels: parser stats, rule stats, queue health, error breakdown)
- Chart rendering: <800ms for time series with 7 days of hourly data (168 data points)
- Table rendering: <300ms for 50 rows with pagination
- Auto-refresh: <300ms to fetch and re-render metrics (delta update, not full reload)

**Drill-Down Performance:**
- Detail panel open animation: <200ms (slide-in from right)
- Drill-down query: <300ms to fetch slowest executions and recommendations
- Histogram rendering: <400ms for 100 bins

**Data Handling:**
- Client-side caching: 60-second TTL for aggregate metrics
- WebSocket updates: Real-time queue depth (30-second fallback polling)
- Pagination: 50 rows per page, <100ms to switch pages

**Optimization Techniques:**
- Virtualized tables: Render only visible rows (50 rows, not 10,000)
- Debounced updates: Auto-refresh throttled to once per 30s (not every second)
- Code splitting: Lazy load detail panels (reduce initial bundle by 40%)
- Web Workers: Heavy calculations (anomaly detection) run off main thread

---

## Edge Cases

**Edge Case 1: Zero Metrics (Empty State)**
- **Scenario:** New system with no metrics yet collected
- **Detection:** API returns empty array `[]` for metrics query
- **Resolution:** Display empty state message: "No metrics available yet. Metrics will appear after first parser execution."
- **Visual:** Illustration + "Getting Started" link to documentation

**Edge Case 2: Clock Skew (Negative Duration)**
- **Scenario:** `completed_at` < `started_at` due to clock skew between servers
- **Detection:** Computed duration < 0
- **Resolution:** Use server-side timestamps only (ignore client timestamps), log warning, display "Invalid duration" with ⚠️ icon
- **Mitigation:** NTP sync on all servers (required)

**Edge Case 3: Metrics During Deployment**
- **Scenario:** Temporary spike in errors/latency during parser deployment
- **Detection:** Error rate >5% but only for 5-minute window (deployment duration)
- **Resolution:** Apply 3-sample moving average to smooth out deployment spikes, show "Deployment in progress" banner
- **Alert Suppression:** Suppress alerts during maintenance window (15 minutes)

**Edge Case 4: High-Cardinality Dimensions**
- **Scenario:** >1M unique parser IDs (multi-tenant with custom parsers)
- **Detection:** Parser stats query returns >1M rows
- **Resolution:** Aggregate by parser_type (exact, regex, fuzzy) instead of parser_id, add filter by parser_id prefix
- **UI:** Show "Viewing aggregated stats (filter to see individual parsers)" message

**Edge Case 5: Data Retention Cutoff**
- **Scenario:** User queries metrics older than 90 days (retention limit)
- **Detection:** Query time_range includes dates before (NOW() - 90 days)
- **Resolution:** Trim query to 90-day window, show info message: "Metrics older than 90 days archived to S3. Contact support for historical data."
- **Action:** Provide S3 export link for archived data

**Edge Case 6: Concurrent Metric Updates**
- **Scenario:** Two API calls update same metric simultaneously (race condition)
- **Detection:** PostgreSQL constraint violation on UNIQUE(id)
- **Resolution:** Use optimistic locking (version column), retry with exponential backoff (3 attempts)
- **User Impact:** None (transparent retry)

**Edge Case 7: Missing Error Context**
- **Scenario:** Error log missing `parser_id` (orphaned error)
- **Detection:** `context.parser_id` is NULL
- **Resolution:** Group under "Unknown Parser" category, log warning for investigation
- **Dashboard:** Show "Unknown Parser" row with count, drill-down to error IDs for debugging

**Edge Case 8: Queue Snapshot During Worker Restart**
- **Scenario:** All workers restart simultaneously, causing temporary spike in "stuck" count
- **Detection:** `stuck` count increases by >50 within 60 seconds
- **Resolution:** Apply 3-sample moving average to smooth out restart spikes, show "Workers restarting" banner
- **Alert Delay:** Wait 5 minutes before triggering stuck documents alert (allow restart to complete)

---

## Conclusion

The Rule Performance & Logs system provides comprehensive observability for document processing pipelines, enabling platform teams to monitor performance, detect issues, optimize slow operations, and ensure SLA compliance. The UX balances real-time monitoring (30-second auto-refresh) with actionable insights (optimization recommendations, drill-down analysis) to support proactive incident response and data-driven optimization decisions across Finance, Healthcare, Legal, Research, E-commerce, SaaS, and Insurance domains.
