# Vertical 1.3: Normalization (Raw ‚Üí Canonical)

**Version**: 1.0
**Status**: Formalized
**Last Updated**: 2025-10-23

---

## Part A: Product Layer (Concrete App Behavior)

### 1. Scope

**What this vertical does:**
- Worker detects `UploadRecord` with `status="parsed"`
- Executes normalizer on raw `ObservationTransaction` records
- Validates and transforms: date ‚Üí ISO 8601, amount ‚Üí signed decimal, description ‚Üí cleaned
- Applies business rules: categorization, merchant normalization, duplicate detection
- Persists validated `CanonicalTransaction` records to `CanonicalStore`
- Writes detailed `NormalizationLog`
- Coordinator updates status to `normalized` or `error`

**Boundaries:**
- Starts: `UploadRecord.status = "parsed"`
- Ends: `UploadRecord.status = "normalized"` (or `"error"`)
- Out of scope: Multi-account reconciliation, transfer linking (vertical 3.5 Relationships or 3.9 Reconciliation Strategies), analytics (vertical 2.x)

**Key principle (from ADR-0004):**
- Raw observations preserved (never modified)
- Canonical records derived (can be regenerated if rules change)

---

### 2. Uso Real (User Flow)

**Automated flow (no user interaction):**

1. **Coordinator detects work:**
   - Query: `SELECT upload_id, observations_count FROM upload_records WHERE status = 'parsed' ORDER BY created_at LIMIT 10`
   - Transition: `status ‚Üí "normalizing"`

2. **Runner loads observations:**
   - Fetch all: `SELECT * FROM observation_store WHERE upload_id = ? ORDER BY row_id`
   - Returns: `ObservationTransaction[]` (raw data from vertical 1.2)

3. **Runner executes normalizer for each observation:**
   - **Date normalization:** `"01/15/2024"` ‚Üí `"2025-01-15T00:00:00Z"` (ISO 8601)
   - **Amount normalization:** `"-5.75"` ‚Üí `-5.75` (decimal with sign)
   - **Description cleaning:** `"  STARBUCKS #1234  "` ‚Üí `"Starbucks #1234"`
   - **Merchant extraction:** `"STARBUCKS #1234"` ‚Üí merchant: `"Starbucks"`, location: `"#1234"`
   - **Currency validation:** Check ISO 4217 compliance
   - **Category inference:** `"Starbucks"` ‚Üí category: `"Food & Drink"` (configurable rules)
   - **Duplicate detection:** Check for same (date, amount, merchant) within N days

4. **Runner persists canonicals:**
   - Upsert to `CanonicalStore` by `canonical_id` (CT_xxx)
   - Each canonical has: normalized fields + metadata (observation_id, confidence_score, applied_rules)

5. **Runner writes NormalizationLog:**
   - Path: `/logs/normalize/{upload_id}.log.json`
   - Contains: normalizer version, execution time, successes, failures, warnings

6. **Coordinator updates status:**
   - Success: `status ‚Üí "normalized"`, set `canonicals_count`, `normalization_log_ref`
   - Partial success: `status ‚Üí "normalized"`, log warnings for failed rows
   - Complete failure: `status ‚Üí "error"`, set `error_message`, `error_code`

7. **Provenance entry:**
   - Event: `normalize.completed` or `normalize.failed`
   - Includes: `upload_id`, `normalizer_version`, `canonicals_count`, `failed_count`

---

### 3. Primitivos Tocados (OL/RL/IL)

**OL (Objective Layer):**
- `UploadRecord` ‚Äî State machine (read status, update to normalized)
- `ObservationStore` ‚Äî Read raw observations (never modified)
- `CanonicalStore` ‚Äî Write normalized transactions (upsert by canonical_id)
- `Normalizer` (interface) ‚Äî Base contract for all normalizers
- `ValidationEngine` ‚Äî Rule-based validation (dates, amounts, currencies)
- `NormalizationRuleSet` ‚Äî Configurable rules (categories, merchants, locales)
- `NormalizationLog` ‚Äî Structured execution log
- `ProvenanceLedger` ‚Äî Record normalize events
- `Coordinator` ‚Äî Status orchestration

**RL (Representation Layer):**
- None (backend-only vertical)

**IL (Interface Layer):**
- None (backend-only vertical)

**New primitives introduced:**
- `Normalizer` ‚Äî Universal interface for raw ‚Üí canonical transformation
- `ValidationEngine` ‚Äî Validate individual fields (date, amount, currency)
- `CanonicalStore` ‚Äî Persistent storage for validated canonicals
- `NormalizationRuleSet` ‚Äî Configuration for domain-specific rules
- `NormalizationLog` ‚Äî Structured log with validation results

---

### 4. Contracts (API + Internal)

#### Internal Worker Contract

**Query for work:**
```sql
SELECT upload_id, observations_count, source_type
FROM upload_records
WHERE status = 'parsed'
ORDER BY created_at
LIMIT 10;
```

**Coordinator transition (before normalizing):**
```python
coordinator.transition(upload_id, to_state="normalizing")
```

**Runner execution:**
```python
def run_normalizer(upload_id: str) -> NormalizationResult:
    # 1. Load observations
    observations = observation_store.get_by_upload(upload_id)

    # 2. Load normalization rules
    upload_record = db.get(UploadRecord, upload_id)
    rules = normalization_rules.get_for_source(upload_record.source_type)

    # 3. Normalize each observation
    canonicals = []
    failures = []

    for obs in observations:
        try:
            canonical = normalizer.normalize(obs, rules)
            canonical_store.upsert(canonical)
            canonicals.append(canonical)
        except ValidationError as e:
            failures.append({
                "observation_id": f"{obs.upload_id}:{obs.row_id}",
                "error": str(e)
            })

    # 4. Write log
    norm_log = create_normalization_log(
        upload_id=upload_id,
        normalizer_version=normalizer.version,
        successes=len(canonicals),
        failures=len(failures),
        failure_details=failures
    )
    write_log(f"/logs/normalize/{upload_id}.log.json", norm_log)

    return NormalizationResult(
        success=len(failures) == 0,
        canonicals_count=len(canonicals),
        failures_count=len(failures),
        normalization_log_ref=f"/logs/normalize/{upload_id}.log.json"
    )
```

**Coordinator transition (after normalizing):**
```python
if result.success:
    coordinator.transition(
        upload_id,
        to_state="normalized",
        metadata={
            "canonicals_count": result.canonicals_count,
            "normalization_log_ref": result.normalization_log_ref
        }
    )
else:
    # Partial failure: some observations normalized, others failed
    if result.canonicals_count > 0:
        coordinator.transition(
            upload_id,
            to_state="normalized",  # Still mark as normalized (partial success)
            metadata={
                "canonicals_count": result.canonicals_count,
                "failures_count": result.failures_count,
                "normalization_log_ref": result.normalization_log_ref
            }
        )
    else:
        # Complete failure: no observations normalized
        coordinator.transition(
            upload_id,
            to_state="error",
            error_code="normalization_failed",
            error_message=f"All {result.failures_count} observations failed normalization"
        )
```

---

### 5. Schemas / Tipos (Persistencia)

**See:**
- `docs/schemas/canonical-transaction.schema.json` ‚Äî Normalized, validated transaction
- `docs/schemas/normalization-log.schema.json` ‚Äî Execution log with validation results

**Key fields in CanonicalTransaction:**
```typescript
interface CanonicalTransaction {
  canonical_id: string        // CT_abc123 (globally unique)
  upload_id: string           // Source upload
  observation_id: string      // upload_id:row_id (link back to raw)

  // Normalized fields
  date: ISO8601DateTime       // "2025-01-15T00:00:00Z" (UTC required, must end with 'Z')
  amount: Decimal             // -5.75 (signed)
  currency: ISO4217           // "USD"
  description: string         // "Starbucks #1234" (cleaned)
  merchant: string            // "Starbucks" (extracted)
  category: string            // "Food & Drink" (inferred)

  // Account mapping
  account: string             // "bofa_debit"

  // Metadata
  confidence_score: number    // 0.0-1.0 (normalization confidence)
  applied_rules: string[]     // ["date_locale_us", "merchant_extract"]
  flags: string[]             // ["possible_duplicate", "large_amount"]

  // Provenance
  normalized_at: ISO8601DateTime  // UTC timestamp (must end with 'Z')
  normalizer_version: string
}
```

**Timezone Enforcement:**
All timestamp fields (`date`, `normalized_at`) must use ISO 8601 format in UTC timezone, ending with 'Z'. Schema validation enforces pattern `".*Z$"`. This ensures audit trail integrity and eliminates timezone ambiguity.

---

### 6. Validaciones & Estados

#### Field-Level Validations

**Date:**
```python
def validate_date(raw_date: str, locale: str) -> datetime:
    """
    Parse and validate date from various formats.

    Supported formats (configurable per source_type):
    - MM/DD/YYYY (US)
    - DD/MM/YYYY (International)
    - YYYY-MM-DD (ISO)
    - "Jan 15, 2024"

    Raises ValidationError if:
    - Unparseable format
    - Date in future (> today + 1 day)
    - Date too old (< 1970-01-01)
    """
    pass
```

**Amount:**
```python
def validate_amount(raw_amount: str) -> Decimal:
    """
    Parse and validate amount from various formats.

    Supported formats:
    - "-50.00" ‚Üí -50.00
    - "(50.00)" ‚Üí -50.00 (accounting notation)
    - "1,234.56" ‚Üí 1234.56
    - "50" ‚Üí 50.00

    Raises ValidationError if:
    - Unparseable format
    - Amount = 0.00 (configurable, may be valid)
    - Amount > 1,000,000 (suspicious, flag as warning)
    """
    pass
```

**Currency:**
```python
def validate_currency(currency: str) -> str:
    """
    Validate ISO 4217 currency code.

    Raises ValidationError if:
    - Not 3 uppercase letters
    - Not in ISO 4217 list
    """
    if currency not in ISO_4217_CODES:
        raise ValidationError(f"Invalid currency: {currency}")
    return currency
```

**Description:**
```python
def clean_description(raw_desc: str) -> str:
    """
    Clean and normalize description.

    Operations:
    - Strip leading/trailing whitespace
    - Collapse multiple spaces ‚Üí single space
    - Remove special characters (configurable)
    - Apply titlecase (configurable)
    """
    return raw_desc.strip().replace("  ", " ")
```

#### Business-Level Validations

**Duplicate detection:**
```python
def detect_duplicate(canonical: CanonicalTransaction) -> bool:
    """
    Check if transaction already exists (potential duplicate).

    Match criteria:
    - Same account
    - Same date (¬±1 day tolerance)
    - Same amount
    - Similar merchant (fuzzy match ‚â•90%)

    Returns: True if duplicate found (adds "possible_duplicate" flag)
    """
    pass
```

**Category inference:**
```python
def infer_category(merchant: str, description: str) -> str:
    """
    Infer transaction category from merchant/description.

    Methods (priority order):
    1. Merchant whitelist: "Starbucks" ‚Üí "Food & Drink"
    2. Keyword matching: "gas station" ‚Üí "Transportation"
    3. MCC code (if available): 5812 ‚Üí "Restaurants"
    4. Default: "Uncategorized"

    Returns: Category string (configurable taxonomy)
    """
    pass
```

**Merchant normalization:**
```python
def normalize_merchant(raw_desc: str) -> tuple[str, Optional[str]]:
    """
    Extract canonical merchant name and location.

    Examples:
    - "STARBUCKS #1234" ‚Üí ("Starbucks", "#1234")
    - "AMAZON.COM*AB12CD" ‚Üí ("Amazon", "order: AB12CD")
    - "SHELL OIL 12345678" ‚Üí ("Shell", None)

    Returns: (merchant_name, location_info)
    """
    pass
```

---

### 7. Edge Cases

**1. Ambiguous date formats:**
- **Input:** `"01/02/2024"` (could be Jan 2 or Feb 1)
- **Handling:** Use locale from `source_type` config (e.g., US sources ‚Üí MM/DD/YYYY)
- **Mitigation:** Parser emits warning if ambiguous, normalizer logs assumption

**2. Negative amount in parentheses:**
- **Input:** `"(50.00)"` (accounting notation for negative)
- **Handling:** Detect parentheses, convert to negative decimal
- **Edge case:** `"(0.00)"` ‚Üí Could be 0 or error, flag as warning

**3. Missing required fields:**
- **Input:** Observation has `amount=""` or `date=null`
- **Handling:** Validation fails, log error, skip canonical creation
- **Result:** UploadRecord still marked `normalized` (partial success)

**4. Future dates:**
- **Input:** `"12/31/2026"` (date in future)
- **Handling:** Allow +1 day tolerance (timezone issues), reject beyond that
- **Flag:** Add `"future_date"` flag if within tolerance

**5. Extreme amounts:**
- **Input:** `"-9999999.99"` (suspiciously large)
- **Handling:** Allow but flag as `"large_amount"` (threshold: >$10k)
- **Rationale:** May be valid (real estate, investments)

**6. Unknown merchants:**
- **Input:** `"XYZ CORP 123456"`
- **Handling:** Keep as-is, category ‚Üí `"Uncategorized"`
- **Learning:** User can manually categorize, rules updated for future

**7. Multiple observations with same data:**
- **Input:** Same transaction appears twice in parsed observations
- **Handling:** Duplicate detection flags second occurrence
- **Result:** Both canonicalized, user can review duplicates in UI

**8. Invalid currency code:**
- **Input:** `"US"` instead of `"USD"`
- **Handling:** Validation fails, log error, skip canonical
- **Mitigation:** Parser should output ISO 4217, but normalizer validates

**9. Re-normalization (rules changed):**
- **Scenario:** Category rules updated, need to re-normalize existing canonicals
- **Handling:** Query all observations for upload_id, re-run normalizer, upsert canonicals (idempotent)
- **Preserved:** Original observations unchanged (raw-first principle)

**10. Partial normalization success:**
- **Scenario:** 100 observations, 95 normalize successfully, 5 fail
- **Handling:** UploadRecord ‚Üí `"normalized"`, log failures, UI shows "95/100 transactions"
- **Rationale:** Don't block entire upload for few bad rows

---

### 8. Acceptance (DoD)

**Success criteria:**

1. **State transition:**
   - ‚úÖ UploadRecord transitions from `parsed` ‚Üí `normalized` (or `error`)
   - ‚úÖ Transition happens ONLY via Coordinator (Runner never touches status)

2. **Canonical creation:**
   - ‚úÖ For each valid ObservationTransaction, CanonicalTransaction created
   - ‚úÖ Canonical has unique `canonical_id` (CT_xxx)
   - ‚úÖ All required fields validated and normalized

3. **Field transformations:**
   - ‚úÖ Dates: All formats ‚Üí ISO 8601 (`YYYY-MM-DDTHH:MM:SSZ`)
   - ‚úÖ Amounts: All formats ‚Üí signed decimal
   - ‚úÖ Descriptions: Trimmed, cleaned
   - ‚úÖ Merchants: Extracted and normalized (where applicable)

4. **Validation enforcement:**
   - ‚úÖ Invalid dates rejected (unparseable, too old, too far in future)
   - ‚úÖ Invalid amounts rejected (unparseable)
   - ‚úÖ Invalid currencies rejected (non-ISO 4217)
   - ‚úÖ Failed validations logged, observation skipped

5. **Idempotency:**
   - ‚úÖ Re-running normalizer produces same canonicals (upsert by canonical_id)
   - ‚úÖ Observations never modified (read-only)

6. **Logging:**
   - ‚úÖ NormalizationLog created with: successes, failures, warnings, execution time
   - ‚úÖ Failed observations include error details
   - ‚úÖ Log persisted before status update

7. **Provenance:**
   - ‚úÖ ProvenanceLedger entry created: `normalize.completed` or `normalize.failed`
   - ‚úÖ Includes: upload_id, normalizer_version, counts

8. **Error handling:**
   - ‚úÖ Partial success allowed (some observations fail, others succeed)
   - ‚úÖ Complete failure ‚Üí status `error`
   - ‚úÖ All errors captured in NormalizationLog

9. **Performance:**
   - ‚úÖ P95 latency: <10s for 100 observations
   - ‚úÖ P95 latency: <60s for 1000 observations
   - ‚úÖ Process 500 observations/second on standard hardware

10. **Configuration:**
    - ‚úÖ Normalization rules externalized (not hardcoded)
    - ‚úÖ Different rules per source_type (locale, categories, merchants)
    - ‚úÖ Rules versioned (changes tracked in ProvenanceLedger)

---

### 9. Logs & Provenance

#### NormalizationLog Structure

**Path:** `/logs/normalize/{upload_id}.log.json`

**Schema:** `docs/schemas/normalization-log.schema.json`

**Example:**
```json
{
  "upload_id": "UL_abc123",
  "normalizer_version": "1.0.0",
  "rule_set_version": "2024.10",
  "started_at": "2025-10-23T14:35:10Z",
  "completed_at": "2025-10-23T14:35:15Z",
  "duration_ms": 5120,
  "observations_processed": 100,
  "canonicals_created": 95,
  "failures": 5,
  "warnings": 12,
  "failure_details": [
    {
      "observation_id": "UL_abc123:42",
      "field": "date",
      "raw_value": "invalid_date",
      "error": "Unparseable date format"
    },
    {
      "observation_id": "UL_abc123:78",
      "field": "currency",
      "raw_value": "US",
      "error": "Invalid ISO 4217 code"
    }
  ],
  "warning_details": [
    {
      "canonical_id": "CT_xyz789",
      "flag": "possible_duplicate",
      "message": "Similar transaction found within 1 day"
    },
    {
      "canonical_id": "CT_abc456",
      "flag": "large_amount",
      "message": "Amount exceeds $10,000"
    }
  ],
  "rules_applied": {
    "date_locale": "en_US",
    "merchant_normalization": true,
    "category_inference": true,
    "duplicate_detection": true
  }
}
```

#### Metrics (Prometheus/StatsD)

```python
# Throughput
normalization.observations.processed.count
normalization.canonicals.created.count
normalization.failures.count

# Latency
normalization.duration.ms (histogram)
normalization.per_observation.ms (histogram)

# Quality
normalization.validation_failures.by_field (counter, labels: field)
normalization.warnings.by_type (counter, labels: warning_type)
normalization.duplicate_rate (gauge)

# Rules
normalization.rule_set.version (gauge, label: version)
normalization.category_coverage (gauge)  # % categorized vs uncategorized
```

#### Logs (Structured JSON)

```json
{
  "timestamp": "2025-10-23T14:35:10Z",
  "level": "INFO",
  "service": "normalization-worker",
  "upload_id": "UL_abc123",
  "event": "normalization.started",
  "observations_count": 100,
  "normalizer_version": "1.0.0"
}

{
  "timestamp": "2025-10-23T14:35:15Z",
  "level": "WARN",
  "service": "normalization-worker",
  "upload_id": "UL_abc123",
  "event": "validation.failed",
  "observation_id": "UL_abc123:42",
  "field": "date",
  "raw_value": "invalid_date",
  "error": "Unparseable date format"
}

{
  "timestamp": "2025-10-23T14:35:15Z",
  "level": "INFO",
  "service": "normalization-worker",
  "upload_id": "UL_abc123",
  "event": "normalization.completed",
  "canonicals_created": 95,
  "failures": 5,
  "duration_ms": 5120
}
```

---

### 10. Riesgos / Diferido

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|-----------|
| **Incorrect date locale assumption** | HIGH - Wrong dates in canonicals | MEDIUM | Emit warnings for ambiguous dates, allow user to specify locale per source |
| **Amount parsing errors** | HIGH - Wrong amounts in canonicals | LOW | Strict validation, reject unparseable amounts, log for manual review |
| **Category inference errors** | MEDIUM - Poor UX, manual recategorization | HIGH | Default to "Uncategorized", allow user to train rules |
| **Duplicate detection false positives** | LOW - User sees duplicate warnings incorrectly | MEDIUM | Flag only, don't block normalization, allow user to confirm/dismiss |
| **Rule changes break re-normalization** | MEDIUM - Can't regenerate canonicals | LOW | Version rule sets, test backwards compatibility |
| **Normalizer crashes on invalid data** | HIGH - Entire upload fails normalization | LOW | Isolate validation per observation, continue on failures |
| **Performance degradation (large files)** | MEDIUM - Slow processing, user frustration | MEDIUM | Batch processing, parallel normalization, optimize validation rules |
| **Merchant normalization inconsistency** | LOW - Same merchant, different canonical names | MEDIUM | Maintain merchant whitelist, fuzzy matching for consistency |

**Key mitigation strategies:**
1. **Fail gracefully:** Allow partial normalization success
2. **Audit trail:** Log all failures and warnings for debugging
3. **User feedback loop:** Let users correct errors, update rules
4. **Idempotency:** Re-normalization safe, original data preserved
5. **Performance:** Batch processing, async workers, indexed queries

---

## Part B: Machinery Layer (Universal Primitives)

### 11. Primitivas Resultantes

#### 11.1 Normalizer (New Primitive)

**Definition:**
Universal interface for transforming raw observations into validated canonical records.

**Contract:**
```python
class Normalizer(ABC):
    @property
    @abstractmethod
    def version(self) -> str:
        """Semantic version of normalizer (e.g., '1.0.0')"""
        pass

    @abstractmethod
    def normalize(
        self,
        observation: ObservationTransaction,
        rules: NormalizationRuleSet
    ) -> CanonicalTransaction:
        """
        Transform raw observation into validated canonical.

        Raises:
            ValidationError: If observation data is invalid
        """
        pass
```

**Responsibilities:**
- Field transformation (date, amount, description)
- Validation (enforce business rules)
- Enrichment (category, merchant, flags)
- Metadata injection (confidence_score, applied_rules)

**NOT responsible for:**
- Status updates (Coordinator only)
- Persistence (done by Runner)
- Multi-transaction logic (transfers, reconciliation ‚Üí vertical 3.5 or 3.9)

---

#### 11.2 ValidationEngine (New Primitive)

**Definition:**
Rule-based validation for individual fields.

**Contract:**
```python
class ValidationEngine:
    def validate_date(self, raw: str, locale: str) -> datetime:
        """Parse and validate date"""
        pass

    def validate_amount(self, raw: str) -> Decimal:
        """Parse and validate amount"""
        pass

    def validate_currency(self, code: str) -> str:
        """Validate ISO 4217 currency"""
        pass

    def validate_description(self, raw: str) -> str:
        """Clean and validate description"""
        pass
```

**Reusable across domains:**
- Finance: transaction validation
- Healthcare: lab result validation (date, numeric values)
- Legal: contract date validation
- Research: citation date validation

---

#### 11.3 CanonicalStore (New Primitive)

**Definition:**
Persistent storage for validated canonical records.

**Contract:**
```python
class CanonicalStore:
    def upsert(self, canonical: CanonicalTransaction) -> None:
        """
        Insert or update canonical by canonical_id (idempotent).
        """
        pass

    def get_by_upload(self, upload_id: str) -> list[CanonicalTransaction]:
        """
        Retrieve all canonicals for an upload.
        """
        pass

    def get_by_id(self, canonical_id: str) -> CanonicalTransaction:
        """
        Retrieve single canonical by ID.
        """
        pass

    def query(self, filters: dict) -> list[CanonicalTransaction]:
        """
        Query canonicals by criteria (date range, account, category, etc.)
        """
        pass
```

**Storage properties:**
- Indexed by canonical_id (primary key)
- Indexed by upload_id (for batch retrieval)
- Indexed by (date, account, amount) (for duplicate detection)
- Supports atomic upserts (idempotent re-normalization)

---

#### 11.4 NormalizationRuleSet (New Primitive)

**Definition:**
Configuration for domain-specific normalization rules.

**Contract:**
```python
@dataclass
class NormalizationRuleSet:
    version: str                    # "2024.10"
    date_locale: str                # "en_US", "es_MX", "fr_FR"
    date_formats: list[str]         # ["MM/DD/YYYY", "DD/MM/YYYY"]
    amount_decimal_separator: str   # "." or ","
    merchant_whitelist: dict        # {"STARBUCKS": "Starbucks"}
    category_rules: list[CategoryRule]
    duplicate_tolerance_days: int   # 1 (allow ¬±1 day for duplicates)
    large_amount_threshold: Decimal # 10000.00
```

**Key principle:**
- Externalized configuration (not hardcoded in normalizer)
- Versioned (changes tracked)
- Per source_type (different rules for BoFA vs Chase)

---

#### 11.5 Existing OL Primitives Used

**UploadRecord:**
- Read: status, observations_count, source_type
- Write: canonicals_count, normalization_log_ref (via Coordinator)

**ObservationStore:**
- Read: raw observations by upload_id
- Never modified (immutable after extraction)

**ProvenanceLedger:**
- Write: normalize.completed or normalize.failed events

**Coordinator:**
- Orchestrate state transitions: parsed ‚Üí normalizing ‚Üí normalized

---

### 12. Interlocks Entre Verticales

**Pattern 1: Runner NEVER updates status**

```python
# ‚úÖ CORRECT
def run_normalizer(upload_id: str):
    # Runner does work
    result = normalizer.normalize(...)
    canonical_store.upsert(...)

    # Return result to Coordinator
    return NormalizationResult(...)

# ‚ùå WRONG
def run_normalizer(upload_id: str):
    result = normalizer.normalize(...)
    db.update(UploadRecord, upload_id, status="normalized")  # NEVER!
```

**Enforcement:**
- Code review: Flag any Runner touching `status` field
- Architecture test: Assert Runner cannot import Coordinator
- Database permissions: Runner has read-only access to `status` column

---

**Pattern 2: Validation failures don't stop processing**

```python
# ‚úÖ CORRECT
canonicals = []
failures = []

for obs in observations:
    try:
        canonical = normalizer.normalize(obs, rules)
        canonicals.append(canonical)
    except ValidationError as e:
        failures.append({"observation_id": obs.id, "error": str(e)})
        continue  # Keep processing other observations

# ‚ùå WRONG
for obs in observations:
    canonical = normalizer.normalize(obs, rules)  # Crash on first error
    canonicals.append(canonical)
```

**Rationale:** Partial success is acceptable (95/100 transactions better than 0/100).

---

**Pattern 3: Observations are immutable**

```python
# ‚úÖ CORRECT
observation = observation_store.get(obs_id)
canonical = normalizer.normalize(observation, rules)  # Read-only
canonical_store.upsert(canonical)  # Write to separate store

# ‚ùå WRONG
observation = observation_store.get(obs_id)
observation.date = normalize_date(observation.date)  # NEVER mutate!
observation_store.update(observation)
```

**Enforcement:**
- ObservationStore only exposes read methods
- Schema: observations table has no UPDATE trigger
- Architecture: ObservationTransaction is frozen dataclass

---

**Pattern 4: Idempotent re-normalization**

```python
# Normalizer is pure function (same input ‚Üí same output)
# Re-running produces same canonical_id ‚Üí upsert is idempotent

# First run
canonical_1 = normalizer.normalize(obs, rules)
canonical_store.upsert(canonical_1)  # INSERT

# Re-run (rules unchanged)
canonical_2 = normalizer.normalize(obs, rules)
canonical_store.upsert(canonical_2)  # UPDATE (same canonical_id)

assert canonical_1.canonical_id == canonical_2.canonical_id
```

**Deterministic canonical_id generation:**
```python
canonical_id = f"CT_{hash(upload_id + row_id + normalizer_version)}"
```

---

### 13. Reusabilidad

**Finance Domain (Current Instantiation):**
- Raw: ObservationTransaction (date, amount, description)
- Canonical: CanonicalTransaction (validated, categorized)
- Rules: US locale, USD currency, merchant normalization

**Healthcare Domain:**
- Raw: ObservationLabResult (test_date, test_name, raw_value, unit)
- Canonical: CanonicalLabResult (validated, normalized units)
- Rules: Date validation, numeric value validation, unit conversion (mg/dL ‚Üí mmol/L)

**Legal Domain:**
- Raw: ObservationContractClause (clause_text, page_number, extracted_date)
- Canonical: CanonicalClause (validated, categorized by clause type)
- Rules: Date validation, clause taxonomy, risk classification

**Research Domain:**
- Raw: ObservationCitation (raw_citation_text, source_url)
- Canonical: CanonicalCitation (author, title, year, DOI)
- Rules: Citation format parsing (APA, MLA, Chicago), DOI validation

**Manufacturing Domain:**
- Raw: ObservationQCMeasurement (timestamp, sensor_reading, unit)
- Canonical: CanonicalMeasurement (validated, normalized units)
- Rules: Timestamp validation, sensor calibration, unit normalization

**Media Domain:**
- Raw: ObservationTranscript (timestamp, speaker, raw_text)
- Canonical: CanonicalUtterance (validated, speaker_id, cleaned_text)
- Rules: Timestamp validation, speaker normalization, text cleaning

---

**Universal Primitive: Normalizer**

```python
# Finance
class FinanceNormalizer(Normalizer):
    def normalize(self, obs: ObservationTransaction, rules: FinanceRuleSet):
        return CanonicalTransaction(...)

# Healthcare
class HealthcareNormalizer(Normalizer):
    def normalize(self, obs: ObservationLabResult, rules: HealthcareRuleSet):
        return CanonicalLabResult(...)

# Legal
class LegalNormalizer(Normalizer):
    def normalize(self, obs: ObservationContractClause, rules: LegalRuleSet):
        return CanonicalClause(...)
```

**Pattern:** Same interface, different domain models and rule sets.

---

### 14. Abstracci√≥n Patr√≥n

**Pattern 1: Two-Stage Storage (Raw + Canonical)**

**Separation:**
- ObservationStore: Immutable raw data
- CanonicalStore: Validated, mutable (can be regenerated)

**Benefits:**
- Re-normalization without re-parsing
- Audit trail (compare raw vs canonical)
- Data recovery (if normalization bugs corrupt canonicals)

**Implementation:**
- Separate tables: `observations`, `canonicals`
- Different indexes: observations by upload_id, canonicals by date/account
- Archival: observations can be archived after N days, canonicals retained

---

**Pattern 2: Configurable Validation Rules**

**Externalization:**
- Rules NOT hardcoded in normalizer
- Rules loaded from config (JSON, database, etc.)
- Rules versioned (changes tracked)

**Example:**
```json
{
  "version": "2024.10",
  "source_type": "bofa_pdf",
  "date_locale": "en_US",
  "date_formats": ["MM/DD/YYYY"],
  "merchant_whitelist": {
    "STARBUCKS": "Starbucks",
    "AMAZON.COM": "Amazon"
  },
  "categories": {
    "Starbucks": "Food & Drink",
    "Amazon": "Shopping"
  }
}
```

**Benefits:**
- Update rules without code changes
- A/B test different rule sets
- Per-user customization (if needed)

---

**Pattern 3: Fail-Safe Processing**

**Principle:** One bad observation doesn't block entire upload.

**Implementation:**
```python
for obs in observations:
    try:
        canonical = normalizer.normalize(obs, rules)
        canonical_store.upsert(canonical)
    except ValidationError as e:
        log_failure(obs, e)
        continue  # Keep going
```

**Result:**
- UploadRecord still marked `normalized` if ANY observations succeed
- Failures logged for manual review
- User sees "95/100 transactions" vs "0/100 transactions"

---

**Pattern 4: Metadata-Rich Canonicals**

**Enrichment:** Canonicals include metadata beyond raw data.

**Fields:**
- `confidence_score`: How confident is normalization? (0.0-1.0)
- `applied_rules`: Which rules were applied? (audit trail)
- `flags`: Warnings or notes (`possible_duplicate`, `large_amount`, etc.)

**Benefits:**
- Observability: Why was transaction categorized as X?
- Quality: Filter low-confidence canonicals for manual review
- Learning: Train better rules from flagged transactions

---

### 15. M√©tricas de Madurez

**Production-ready normalization:**

- [x] **Normalizer interface defined** ‚Äî Universal contract for all domains
- [x] **ValidationEngine primitive** ‚Äî Field-level validation (date, amount, currency)
- [x] **CanonicalStore primitive** ‚Äî Persistent storage with idempotent upserts
- [x] **NormalizationRuleSet** ‚Äî Externalized, versioned configuration
- [x] **Partial success handling** ‚Äî Don't block entire upload for bad rows
- [x] **Immutable observations** ‚Äî Raw data never modified
- [x] **Idempotent re-normalization** ‚Äî Same input ‚Üí same output
- [x] **Structured logging** ‚Äî NormalizationLog with failures and warnings
- [x] **Provenance tracking** ‚Äî ProvenanceLedger entries for all normalizations
- [x] **Runner/Coordinator split enforced** ‚Äî Runner never touches status
- [ ] **Rule versioning implemented** ‚Äî Track rule changes in ProvenanceLedger
- [ ] **Performance benchmarks met** ‚Äî 500 obs/sec on standard hardware
- [ ] **Duplicate detection tested** ‚Äî False positive rate <5%
- [ ] **Category inference accuracy** ‚Äî Measured against user corrections
- [ ] **Multi-locale support tested** ‚Äî US, MX, EU date formats
- [ ] **Error recovery tested** ‚Äî Re-normalization after rule changes
- [ ] **Load testing completed** ‚Äî 10k observations/upload, 100 concurrent uploads

---

## Part C: Cross-Cutting Concerns

### 16. Security Considerations

**Data integrity:**
- **Validation strictness:** Reject invalid data (unparseable dates, amounts)
- **SQL injection:** Use parameterized queries for all database operations
- **Input sanitization:** Clean descriptions (remove special characters if needed)

**Access control:**
- **Worker permissions:** Read-only access to observations, write to canonicals
- **Coordinator permissions:** Write access to UploadRecord.status
- **Audit:** All normalization events logged in ProvenanceLedger

**Sensitive data handling:**
- **PII in descriptions:** Do NOT log raw descriptions in metrics (may contain merchant details)
- **Amount privacy:** Aggregate metrics OK, individual amounts logged only in secure logs

**Configuration security:**
- **Rule tampering:** NormalizationRuleSet signed/checksummed (detect unauthorized changes)
- **Version control:** All rule changes tracked in git, deployed via CI/CD

---

### 17. Performance Characteristics

**Latency targets:**
- **P50:** <5s for 100 observations
- **P95:** <10s for 100 observations
- **P99:** <20s for 100 observations
- **P95:** <60s for 1000 observations

**Throughput targets:**
- **Per worker:** 500 observations/second
- **System:** 10,000 observations/second (20 workers)

**Optimization strategies:**

1. **Batch processing:**
   - Fetch all observations for upload in single query
   - Bulk upsert canonicals (transaction batch)

2. **Parallel normalization:**
   - Each observation independent ‚Üí parallelize with thread pool
   - Example: 100 observations, 10 threads ‚Üí 10s vs 50s

3. **Index optimization:**
   - ObservationStore: Index on upload_id
   - CanonicalStore: Composite index on (date, account, amount) for duplicate detection

4. **Rule caching:**
   - Load NormalizationRuleSet once, reuse for all observations in upload
   - Cache merchant whitelist in memory (LRU cache)

5. **Lazy validation:**
   - Skip duplicate detection if flag disabled in rules
   - Skip category inference if not needed

**Degradation scenarios:**
- **Large uploads (10k+ observations):** Queue for async processing, show progress bar
- **Rule complexity:** Profile validation rules, optimize slow patterns
- **Database contention:** Retry with exponential backoff

---

### 18. Observability

**Key metrics:**

1. **Throughput:**
   - `normalization.observations.processed.rate` (obs/sec)
   - `normalization.canonicals.created.rate` (canonicals/sec)

2. **Latency:**
   - `normalization.duration.p50` (ms)
   - `normalization.duration.p95` (ms)
   - `normalization.per_observation.p95` (ms)

3. **Quality:**
   - `normalization.failure_rate` (%)
   - `normalization.validation_failures.by_field` (counter, label: field)
   - `normalization.duplicate_rate` (%)
   - `normalization.category_coverage` (% categorized)

4. **Rules:**
   - `normalization.rule_set.version` (gauge, current version)
   - `normalization.rule_changes.count` (counter, rule updates)

**Dashboards:**

**Dashboard 1: Normalization Health**
- Success rate (95%+ target)
- Latency (P50, P95, P99)
- Failure breakdown (by field: date, amount, currency)
- Throughput (observations/sec)

**Dashboard 2: Quality Metrics**
- Category coverage (% uncategorized)
- Duplicate detection rate
- Large amount flags (% transactions >$10k)
- Low confidence transactions (confidence <0.7)

**Dashboard 3: Performance**
- Worker utilization
- Queue depth (uploads waiting for normalization)
- Batch size distribution
- Database query latency

**Alerts:**

```yaml
# Critical
- alert: NormalizationFailureRateHigh
  expr: normalization.failure_rate > 0.10  # >10% failure
  severity: critical

- alert: NormalizationLatencyHigh
  expr: normalization.duration.p95 > 20000  # >20s for 100 obs
  severity: critical

# Warning
- alert: CategoryCoverageLow
  expr: normalization.category_coverage < 0.70  # <70% categorized
  severity: warning

- alert: DuplicateRateHigh
  expr: normalization.duplicate_rate > 0.15  # >15% duplicates
  severity: warning
```

---

### 19. Testing Strategy (Detailed)

**Unit tests:**

1. **ValidationEngine:**
   - Test date parsing (all supported formats)
   - Test amount parsing (negatives, parentheses, commas)
   - Test currency validation (valid/invalid ISO codes)
   - Test description cleaning (whitespace, special chars)

2. **Normalizer:**
   - Test field transformations (raw ‚Üí canonical)
   - Test validation error handling
   - Test confidence scoring
   - Test flag generation (duplicates, large amounts)

3. **CanonicalStore:**
   - Test idempotent upserts (same canonical_id)
   - Test query operations (by upload, by date range)
   - Test duplicate detection

**Integration tests:**

1. **End-to-end normalization:**
   - Upload file ‚Üí parse ‚Üí normalize ‚Üí verify canonicals
   - Test state transitions (parsed ‚Üí normalizing ‚Üí normalized)
   - Test partial success (some observations fail)
   - Test complete failure (all observations fail)

2. **Re-normalization:**
   - Normalize observations with rule set v1
   - Update rules to v2
   - Re-normalize same observations
   - Verify canonicals updated correctly

3. **Error scenarios:**
   - Invalid dates ‚Üí validation failure logged
   - Invalid currency ‚Üí observation skipped
   - Missing required fields ‚Üí observation skipped
   - Normalizer crash ‚Üí coordinator marks error

**Golden data tests:**

```python
# Given: Raw observation (known input)
observation = ObservationTransaction(
    upload_id="UL_test",
    row_id=0,
    date="01/15/2024",
    amount="-5.75",
    description="  STARBUCKS #1234  ",
    currency="USD",
    account="bofa_debit",
    source_type="bofa_pdf",
    parser_id="bofa_pdf_parser",
    parser_version="1.0.0",
    extracted_at="2025-10-23T14:00:00Z"
)

# When: Normalize
rules = NormalizationRuleSet(date_locale="en_US", ...)
canonical = normalizer.normalize(observation, rules)

# Then: Expect canonical (known output)
assert canonical.date == "2025-01-15T00:00:00Z"
assert canonical.amount == Decimal("-5.75")
assert canonical.description == "Starbucks #1234"
assert canonical.merchant == "Starbucks"
assert canonical.category == "Food & Drink"
assert canonical.confidence_score >= 0.9
```

**Performance tests:**

- **Benchmark:** Normalize 100 observations, measure latency
- **Load test:** Normalize 10k observations, verify no memory leaks
- **Concurrency test:** 10 workers normalizing simultaneously

---

### 20. Operations Runbook

**Deployment:**

1. **Normalizer versioning:**
   - Deploy new normalizer version with feature flags
   - Canary deployment (5% of uploads use new version)
   - Monitor metrics, rollback if failure rate increases

2. **Rule updates:**
   - Update NormalizationRuleSet in config store
   - Increment version (2024.10 ‚Üí 2024.11)
   - ProvenanceLedger logs rule version for each normalization

3. **Database migrations:**
   - Add new canonical fields (e.g., `merchant_canonical`)
   - Backfill existing canonicals (re-normalize)
   - No downtime (blue-green deployment)

**Monitoring:**

- **Worker health:** Heartbeat checks, restart on failure
- **Queue depth:** Alert if >100 uploads waiting for normalization
- **Database load:** Monitor query latency, index usage

**Debugging:**

1. **Failed normalization:**
   - Read NormalizationLog: `/logs/normalize/{upload_id}.log.json`
   - Identify failed observation_id
   - Retrieve raw observation from ObservationStore
   - Reproduce locally with same rules

2. **Incorrect canonical:**
   - Compare canonical vs observation
   - Check applied_rules (which rules were used?)
   - Verify rule set version (is it outdated?)
   - Manually re-normalize to verify

3. **Performance issues:**
   - Profile normalizer (which validation is slow?)
   - Check database query logs (slow queries?)
   - Analyze batch sizes (too large?)

**Runbooks:**

**Runbook 1: High failure rate**
1. Check NormalizationLog for common errors
2. If date parsing errors: Verify locale config
3. If currency errors: Check parser output
4. If all errors same field: Investigate validation rule change

**Runbook 2: Slow normalization**
1. Check worker CPU/memory usage
2. Analyze query latency (duplicate detection slow?)
3. Check batch size (split large uploads?)
4. Scale workers horizontally (if queue backed up)

**Runbook 3: Incorrect categories**
1. Retrieve canonical and observation
2. Check merchant extraction (correct merchant name?)
3. Verify category rules (is merchant in whitelist?)
4. Update rules, re-normalize

---

## üíª Tech Stack (Finance-App Implementation)

**Scale Target:** ~195 transactions/month from 6 accounts (BofA, Apple Card, Scotia, Wise, Stripe, BofA Credit)

### Backend

**Normalizer Implementation (TransactionNormalizer):**
- **Library:** Python `decimal.Decimal` for amount precision
- **Type:** `Generic[ObservationTransaction, CanonicalTransaction]` (P1 fix applied)
- **Validation steps:**
  1. Date parsing: `dateutil.parser.parse()` with locale support (MM/DD/YYYY for US banks)
  2. Amount parsing: Decimal string (`"-123.45"`) ‚Üí `Decimal("-123.45")`
  3. Currency extraction: From metadata or default to USD
  4. Merchant normalization: Apply rules from NormalizationRuleStore
  5. Category assignment: Default "uncategorized" (categorization in separate vertical)
- **Performance:** <50ms per transaction (p95)

**ValidationEngine Implementation:**
- **Date validation:**
  - Parse with multiple formats: `MM/DD/YYYY`, `YYYY-MM-DD`, `DD/MM/YYYY`
  - Reject future dates (> today + 1 day)
  - Reject dates before 1900
- **Amount validation:**
  - Must be valid decimal: `-?[0-9]+(\\.[0-9]{1,2})?`
  - Reject NaN, Infinity
  - Max precision: 2 decimal places
- **Currency validation:**
  - Must be ISO 4217 code (USD, MXN, EUR, etc.)
  - Use `currencies` Python library for validation
- **Duplicate detection:**
  - Query: `SELECT * FROM canonical_transactions WHERE account = ? AND date = ? AND amount = ? AND description = ?`
  - If exists: Return existing canonical_id (idempotent)
  - If not: Generate new UUIDv5 canonical_id

**CanonicalStore Implementation:**
- **Database:** SQLite
  - Table: `canonical_transactions`
  - Columns: `canonical_id`, `user_id`, `account`, `date`, `amount`, `currency`, `merchant`, `description`, `category`, `metadata`, `created_at`, `updated_at`, `version`
- **Indexing:**
  - `(user_id, date)` for date-range queries
  - `(user_id, account, date, amount)` for duplicate detection
  - `(canonical_id)` unique constraint
- **Storage:** ~800 bytes/transaction √ó 195 tx/month = 156KB/month = 1.87MB/year

**NormalizationRuleStore Implementation:**
- **Format:** YAML files (not database)
  - Path: `/config/normalization_rules/{source_type}.yaml`
  - Version control: Git tracks rule changes
  - Hot-reload: Watch file changes, reload rules without restart
- **Example rule file (`bofa_pdf.yaml`):**
  ```yaml
  version: "1.0"
  rules:
    merchants:
      - pattern: "AMAZON\\.COM.*"
        replacement: "Amazon"
        type: "regex"
        priority: 10
      - pattern: "UBER EATS"
        replacement: "Uber Eats"
        type: "exact"
        priority: 5
    currencies:
      default: "USD"
  ```
- **Rationale:** YAML > UI editor for 500 tx/mes scale (simpler, version-controlled, no DB overhead)

**NormalizationLog Implementation:**
- **Database:** SQLite
  - Table: `normalization_logs`
  - Columns: `log_id`, `upload_id`, `observations_processed`, `canonicals_created`, `validation_errors`, `started_at`, `completed_at`
- **Structured format:**
  ```json
  {
    "log_id": "norm_abc123",
    "upload_id": "UL_abc123",
    "observations_processed": 48,
    "canonicals_created": 45,
    "validation_errors": [
      {"row": 12, "field": "date", "error": "Invalid date format", "value": "10-32-2024"},
      {"row": 23, "field": "amount", "error": "Not a valid decimal", "value": "N/A"}
    ],
    "started_at": "2025-10-23T14:33:01Z",
    "completed_at": "2025-10-23T14:33:03Z"
  }
  ```

### Frontend

**No new IL components for this vertical** - Normalization runs server-side after parsing. User sees final canonical transactions in Transaction List (Vertical 2.1).

### API Endpoints

**GET /api/normalization-logs/:upload_id**
- **Purpose:** Get normalization log for debugging failed validations
- **Auth:** User session (only owner)
- **Response:** `{ upload_id, observations_processed, canonicals_created, validation_errors }`
- **Latency:** <50ms (single SELECT)

**POST /api/uploads/:upload_id/renormalize** (Admin only, future)
- **Purpose:** Re-run normalization after rule changes
- **Auth:** Admin role
- **Body:** `{ force: true }`
- **Response:** `{ success: true, canonicals_created }`

### Database Schema

```sql
-- CanonicalTransaction (validated, normalized)
CREATE TABLE canonical_transactions (
    canonical_id TEXT PRIMARY KEY,  -- UUIDv5(user_id + account + date + amount + description)
    user_id TEXT NOT NULL,
    account TEXT NOT NULL,  -- "BofA Checking", "Wise USD", etc.
    date TEXT NOT NULL,  -- ISO 8601 (YYYY-MM-DD)
    amount TEXT NOT NULL,  -- Decimal string (e.g., "-123.45")
    currency TEXT NOT NULL,  -- ISO 4217 (USD, MXN, EUR)
    merchant TEXT,  -- Normalized merchant name (e.g., "Amazon")
    description TEXT NOT NULL,  -- Original description
    category TEXT,  -- "uncategorized" by default (categorization in vertical 3.4)
    metadata TEXT,  -- JSON (observation_id, upload_id, normalization_version)
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    version INTEGER NOT NULL DEFAULT 1  -- Optimistic locking
);

-- Indexes for query performance
CREATE INDEX idx_canonical_user_date ON canonical_transactions(user_id, date);
CREATE INDEX idx_canonical_duplicate_check ON canonical_transactions(user_id, account, date, amount);
CREATE INDEX idx_canonical_merchant ON canonical_transactions(merchant);

-- NormalizationLog
CREATE TABLE normalization_logs (
    log_id TEXT PRIMARY KEY,  -- norm_xxxxx
    upload_id TEXT NOT NULL UNIQUE REFERENCES upload_records(upload_id),
    observations_processed INTEGER NOT NULL,
    canonicals_created INTEGER NOT NULL,
    validation_errors TEXT,  -- JSON array
    started_at TEXT NOT NULL,
    completed_at TEXT
);

CREATE INDEX idx_normalization_upload ON normalization_logs(upload_id);
```

### Normalization Rules Example

**`/config/normalization_rules/bofa_pdf.yaml`:**
```yaml
version: "1.0"
source_type: "bofa_pdf"

# Merchant normalization rules
merchants:
  - pattern: "AMAZON\\.COM.*"
    replacement: "Amazon"
    type: "regex"
    priority: 10
  - pattern: "AMZN MKTP US.*"
    replacement: "Amazon"
    type: "regex"
    priority: 10
  - pattern: "UBER EATS.*"
    replacement: "Uber Eats"
    type: "regex"
    priority: 5
  - pattern: "WALMART"
    replacement: "Walmart"
    type: "exact"
    priority: 5

# Currency defaults
currencies:
  default: "USD"

# Validation settings
validation:
  allow_future_dates: false
  max_amount: 1000000.00  # $1M
  min_amount: -1000000.00
```

### Testing

**Unit Tests:**
- ValidationEngine date parsing (multiple formats)
- ValidationEngine amount parsing (edge cases: 0.00, -0.01, 999999.99)
- Normalizer merchant rules (regex, exact match, priority)
- CanonicalStore duplicate detection (same tx uploaded twice)
- NormalizationLog validation error tracking

**Integration Tests:**
- Parse upload ‚Üí Normalize ‚Üí Canonicals created ‚Üí NormalizationLog written
- Re-normalize upload ‚Üí Duplicate detection ‚Üí Same canonical_id returned
- Invalid observation ‚Üí Validation error logged ‚Üí Partial success (45/48 created)

**Golden Data Tests:**
- `tests/fixtures/bofa_observations.json` ‚Üí Expected 48 canonicals
- Compare normalized canonicals vs expected (date ISO 8601, amount Decimal, merchant normalized)

### Performance Characteristics

**Latency Targets:**
- Validation: <10ms per observation (date + amount + currency + duplicate check)
- Merchant normalization: <5ms per observation (YAML rules cached in memory)
- CanonicalStore bulk insert: <100ms for 50 transactions
- **Total normalization latency:** <3s for 50 observations (p95)

**Scalability:**
- Current: ~195 tx/month = 2,340 tx/year
- Storage: ~800 bytes/tx √ó 2,340 = 1.87MB/year
- 5-year projection: 11,700 tx = 9.4MB total (trivial for SQLite)

**Robustness:**
- Partial success: 45/48 valid ‚Üí Create 45 canonicals, log 3 errors (don't fail entire batch)
- Idempotency: Re-normalize same upload ‚Üí Same canonical_ids (duplicate detection)
- Hot-reload rules: Edit YAML ‚Üí Rules reload without restart

---

## Summary

**Vertical 1.3 (Normalization) transforms raw observations into validated canonical transactions.**

**Key flows:**
1. Coordinator detects `status="parsed"`
2. Runner loads observations, applies normalizer
3. Normalizer validates and transforms each observation
4. Canonicals persisted to CanonicalStore
5. NormalizationLog written
6. Coordinator updates to `status="normalized"`

**New primitives:**
- **Normalizer** ‚Äî Universal interface for raw ‚Üí canonical transformation
- **ValidationEngine** ‚Äî Field-level validation (date, amount, currency)
- **CanonicalStore** ‚Äî Persistent storage for validated canonicals
- **NormalizationRuleSet** ‚Äî Externalized, versioned configuration

**Key patterns:**
- **Fail-safe:** Partial success allowed (don't block for bad rows)
- **Immutable observations:** Raw data never modified
- **Idempotent re-normalization:** Same input ‚Üí same output
- **Configurable rules:** Not hardcoded, externalized and versioned

**Multi-domain applicability:**
- Finance: transactions
- Healthcare: lab results
- Legal: contract clauses
- Research: citations
- Manufacturing: QC measurements

**Next vertical:** 2.1 (Transaction List View) ‚Äî View, filter, and explore canonical transactions with pagination and indexing.
