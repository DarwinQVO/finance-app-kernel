# Vertical 1.3: Normalization (Raw → Canonical)

**Version**: 1.0
**Status**: Formalized
**Last Updated**: 2025-10-23

---

## Part A: Product Layer (Concrete App Behavior)

### 1. Scope

**What this vertical does:**
- Worker detects `UploadRecord` with `status="parsed"`
- Executes normalizer on raw `ObservationTransaction` records
- Validates and transforms: date → ISO 8601, amount → signed decimal, description → cleaned
- Applies business rules: categorization, merchant normalization, duplicate detection
- Persists validated `CanonicalTransaction` records to `CanonicalStore`
- Writes detailed `NormalizationLog`
- Coordinator updates status to `normalized` or `error`

**Boundaries:**
- Starts: `UploadRecord.status = "parsed"`
- Ends: `UploadRecord.status = "normalized"` (or `"error"`)
- Out of scope: Multi-account reconciliation, transfer linking (vertical 1.4), analytics (vertical 2.x)

**Key principle (from ADR-0004):**
- Raw observations preserved (never modified)
- Canonical records derived (can be regenerated if rules change)

---

### 2. Uso Real (User Flow)

**Automated flow (no user interaction):**

1. **Coordinator detects work:**
   - Query: `SELECT upload_id, observations_count FROM upload_records WHERE status = 'parsed' ORDER BY created_at LIMIT 10`
   - Transition: `status → "normalizing"`

2. **Runner loads observations:**
   - Fetch all: `SELECT * FROM observation_store WHERE upload_id = ? ORDER BY row_id`
   - Returns: `ObservationTransaction[]` (raw data from vertical 1.2)

3. **Runner executes normalizer for each observation:**
   - **Date normalization:** `"01/15/2024"` → `"2025-01-15T00:00:00Z"` (ISO 8601)
   - **Amount normalization:** `"-5.75"` → `-5.75` (decimal with sign)
   - **Description cleaning:** `"  STARBUCKS #1234  "` → `"Starbucks #1234"`
   - **Merchant extraction:** `"STARBUCKS #1234"` → merchant: `"Starbucks"`, location: `"#1234"`
   - **Currency validation:** Check ISO 4217 compliance
   - **Category inference:** `"Starbucks"` → category: `"Food & Drink"` (configurable rules)
   - **Duplicate detection:** Check for same (date, amount, merchant) within N days

4. **Runner persists canonicals:**
   - Upsert to `CanonicalStore` by `canonical_id` (CT_xxx)
   - Each canonical has: normalized fields + metadata (observation_id, confidence_score, applied_rules)

5. **Runner writes NormalizationLog:**
   - Path: `/logs/normalize/{upload_id}.log.json`
   - Contains: normalizer version, execution time, successes, failures, warnings

6. **Coordinator updates status:**
   - Success: `status → "normalized"`, set `canonicals_count`, `normalization_log_ref`
   - Partial success: `status → "normalized"`, log warnings for failed rows
   - Complete failure: `status → "error"`, set `error_message`, `error_code`

7. **Provenance entry:**
   - Event: `normalize.completed` or `normalize.failed`
   - Includes: `upload_id`, `normalizer_version`, `canonicals_count`, `failed_count`

---

### 3. Primitivos Tocados (OL/RL/IL)

**OL (Objective Layer):**
- `UploadRecord` — State machine (read status, update to normalized)
- `ObservationStore` — Read raw observations (never modified)
- `CanonicalStore` — Write normalized transactions (upsert by canonical_id)
- `Normalizer` (interface) — Base contract for all normalizers
- `ValidationEngine` — Rule-based validation (dates, amounts, currencies)
- `NormalizationRuleSet` — Configurable rules (categories, merchants, locales)
- `NormalizationLog` — Structured execution log
- `ProvenanceLedger` — Record normalize events
- `Coordinator` — Status orchestration

**RL (Representation Layer):**
- None (backend-only vertical)

**IL (Interface Layer):**
- None (backend-only vertical)

**New primitives introduced:**
- `Normalizer` — Universal interface for raw → canonical transformation
- `ValidationEngine` — Validate individual fields (date, amount, currency)
- `CanonicalStore` — Persistent storage for validated canonicals
- `NormalizationRuleSet` — Configuration for domain-specific rules
- `NormalizationLog` — Structured log with validation results

---

### 4. Contracts (API + Internal)

#### Internal Worker Contract

**Query for work:**
```sql
SELECT upload_id, observations_count, source_type
FROM upload_records
WHERE status = 'parsed'
ORDER BY created_at
LIMIT 10;
```

**Coordinator transition (before normalizing):**
```python
coordinator.transition(upload_id, to_state="normalizing")
```

**Runner execution:**
```python
def run_normalizer(upload_id: str) -> NormalizationResult:
    # 1. Load observations
    observations = observation_store.get_by_upload(upload_id)

    # 2. Load normalization rules
    upload_record = db.get(UploadRecord, upload_id)
    rules = normalization_rules.get_for_source(upload_record.source_type)

    # 3. Normalize each observation
    canonicals = []
    failures = []

    for obs in observations:
        try:
            canonical = normalizer.normalize(obs, rules)
            canonical_store.upsert(canonical)
            canonicals.append(canonical)
        except ValidationError as e:
            failures.append({
                "observation_id": f"{obs.upload_id}:{obs.row_id}",
                "error": str(e)
            })

    # 4. Write log
    norm_log = create_normalization_log(
        upload_id=upload_id,
        normalizer_version=normalizer.version,
        successes=len(canonicals),
        failures=len(failures),
        failure_details=failures
    )
    write_log(f"/logs/normalize/{upload_id}.log.json", norm_log)

    return NormalizationResult(
        success=len(failures) == 0,
        canonicals_count=len(canonicals),
        failures_count=len(failures),
        normalization_log_ref=f"/logs/normalize/{upload_id}.log.json"
    )
```

**Coordinator transition (after normalizing):**
```python
if result.success:
    coordinator.transition(
        upload_id,
        to_state="normalized",
        metadata={
            "canonicals_count": result.canonicals_count,
            "normalization_log_ref": result.normalization_log_ref
        }
    )
else:
    # Partial failure: some observations normalized, others failed
    if result.canonicals_count > 0:
        coordinator.transition(
            upload_id,
            to_state="normalized",  # Still mark as normalized (partial success)
            metadata={
                "canonicals_count": result.canonicals_count,
                "failures_count": result.failures_count,
                "normalization_log_ref": result.normalization_log_ref
            }
        )
    else:
        # Complete failure: no observations normalized
        coordinator.transition(
            upload_id,
            to_state="error",
            error_code="normalization_failed",
            error_message=f"All {result.failures_count} observations failed normalization"
        )
```

---

### 5. Schemas (JSON Schema)

**See:**
- `docs/schemas/canonical-transaction.schema.json` — Normalized, validated transaction
- `docs/schemas/normalization-log.schema.json` — Execution log with validation results

**Key fields in CanonicalTransaction:**
```typescript
interface CanonicalTransaction {
  canonical_id: string        // CT_abc123 (globally unique)
  upload_id: string           // Source upload
  observation_id: string      // upload_id:row_id (link back to raw)

  // Normalized fields
  date: ISO8601DateTime       // "2025-01-15T00:00:00Z"
  amount: Decimal             // -5.75 (signed)
  currency: ISO4217           // "USD"
  description: string         // "Starbucks #1234" (cleaned)
  merchant: string            // "Starbucks" (extracted)
  category: string            // "Food & Drink" (inferred)

  // Account mapping
  account: string             // "bofa_debit"

  // Metadata
  confidence_score: number    // 0.0-1.0 (normalization confidence)
  applied_rules: string[]     // ["date_locale_us", "merchant_extract"]
  flags: string[]             // ["possible_duplicate", "large_amount"]

  // Provenance
  normalized_at: ISO8601DateTime
  normalizer_version: string
}
```

---

### 6. Validaciones (Field-Level + Business-Level)

#### Field-Level Validations

**Date:**
```python
def validate_date(raw_date: str, locale: str) -> datetime:
    """
    Parse and validate date from various formats.

    Supported formats (configurable per source_type):
    - MM/DD/YYYY (US)
    - DD/MM/YYYY (International)
    - YYYY-MM-DD (ISO)
    - "Jan 15, 2024"

    Raises ValidationError if:
    - Unparseable format
    - Date in future (> today + 1 day)
    - Date too old (< 1970-01-01)
    """
    pass
```

**Amount:**
```python
def validate_amount(raw_amount: str) -> Decimal:
    """
    Parse and validate amount from various formats.

    Supported formats:
    - "-50.00" → -50.00
    - "(50.00)" → -50.00 (accounting notation)
    - "1,234.56" → 1234.56
    - "50" → 50.00

    Raises ValidationError if:
    - Unparseable format
    - Amount = 0.00 (configurable, may be valid)
    - Amount > 1,000,000 (suspicious, flag as warning)
    """
    pass
```

**Currency:**
```python
def validate_currency(currency: str) -> str:
    """
    Validate ISO 4217 currency code.

    Raises ValidationError if:
    - Not 3 uppercase letters
    - Not in ISO 4217 list
    """
    if currency not in ISO_4217_CODES:
        raise ValidationError(f"Invalid currency: {currency}")
    return currency
```

**Description:**
```python
def clean_description(raw_desc: str) -> str:
    """
    Clean and normalize description.

    Operations:
    - Strip leading/trailing whitespace
    - Collapse multiple spaces → single space
    - Remove special characters (configurable)
    - Apply titlecase (configurable)
    """
    return raw_desc.strip().replace("  ", " ")
```

#### Business-Level Validations

**Duplicate detection:**
```python
def detect_duplicate(canonical: CanonicalTransaction) -> bool:
    """
    Check if transaction already exists (potential duplicate).

    Match criteria:
    - Same account
    - Same date (±1 day tolerance)
    - Same amount
    - Similar merchant (fuzzy match ≥90%)

    Returns: True if duplicate found (adds "possible_duplicate" flag)
    """
    pass
```

**Category inference:**
```python
def infer_category(merchant: str, description: str) -> str:
    """
    Infer transaction category from merchant/description.

    Methods (priority order):
    1. Merchant whitelist: "Starbucks" → "Food & Drink"
    2. Keyword matching: "gas station" → "Transportation"
    3. MCC code (if available): 5812 → "Restaurants"
    4. Default: "Uncategorized"

    Returns: Category string (configurable taxonomy)
    """
    pass
```

**Merchant normalization:**
```python
def normalize_merchant(raw_desc: str) -> tuple[str, Optional[str]]:
    """
    Extract canonical merchant name and location.

    Examples:
    - "STARBUCKS #1234" → ("Starbucks", "#1234")
    - "AMAZON.COM*AB12CD" → ("Amazon", "order: AB12CD")
    - "SHELL OIL 12345678" → ("Shell", None)

    Returns: (merchant_name, location_info)
    """
    pass
```

---

### 7. Edge Cases

**1. Ambiguous date formats:**
- **Input:** `"01/02/2024"` (could be Jan 2 or Feb 1)
- **Handling:** Use locale from `source_type` config (e.g., US sources → MM/DD/YYYY)
- **Mitigation:** Parser emits warning if ambiguous, normalizer logs assumption

**2. Negative amount in parentheses:**
- **Input:** `"(50.00)"` (accounting notation for negative)
- **Handling:** Detect parentheses, convert to negative decimal
- **Edge case:** `"(0.00)"` → Could be 0 or error, flag as warning

**3. Missing required fields:**
- **Input:** Observation has `amount=""` or `date=null`
- **Handling:** Validation fails, log error, skip canonical creation
- **Result:** UploadRecord still marked `normalized` (partial success)

**4. Future dates:**
- **Input:** `"12/31/2026"` (date in future)
- **Handling:** Allow +1 day tolerance (timezone issues), reject beyond that
- **Flag:** Add `"future_date"` flag if within tolerance

**5. Extreme amounts:**
- **Input:** `"-9999999.99"` (suspiciously large)
- **Handling:** Allow but flag as `"large_amount"` (threshold: >$10k)
- **Rationale:** May be valid (real estate, investments)

**6. Unknown merchants:**
- **Input:** `"XYZ CORP 123456"`
- **Handling:** Keep as-is, category → `"Uncategorized"`
- **Learning:** User can manually categorize, rules updated for future

**7. Multiple observations with same data:**
- **Input:** Same transaction appears twice in parsed observations
- **Handling:** Duplicate detection flags second occurrence
- **Result:** Both canonicalized, user can review duplicates in UI

**8. Invalid currency code:**
- **Input:** `"US"` instead of `"USD"`
- **Handling:** Validation fails, log error, skip canonical
- **Mitigation:** Parser should output ISO 4217, but normalizer validates

**9. Re-normalization (rules changed):**
- **Scenario:** Category rules updated, need to re-normalize existing canonicals
- **Handling:** Query all observations for upload_id, re-run normalizer, upsert canonicals (idempotent)
- **Preserved:** Original observations unchanged (raw-first principle)

**10. Partial normalization success:**
- **Scenario:** 100 observations, 95 normalize successfully, 5 fail
- **Handling:** UploadRecord → `"normalized"`, log failures, UI shows "95/100 transactions"
- **Rationale:** Don't block entire upload for few bad rows

---

### 8. Criterios de Aceptación

**Success criteria:**

1. **State transition:**
   - ✅ UploadRecord transitions from `parsed` → `normalized` (or `error`)
   - ✅ Transition happens ONLY via Coordinator (Runner never touches status)

2. **Canonical creation:**
   - ✅ For each valid ObservationTransaction, CanonicalTransaction created
   - ✅ Canonical has unique `canonical_id` (CT_xxx)
   - ✅ All required fields validated and normalized

3. **Field transformations:**
   - ✅ Dates: All formats → ISO 8601 (`YYYY-MM-DDTHH:MM:SSZ`)
   - ✅ Amounts: All formats → signed decimal
   - ✅ Descriptions: Trimmed, cleaned
   - ✅ Merchants: Extracted and normalized (where applicable)

4. **Validation enforcement:**
   - ✅ Invalid dates rejected (unparseable, too old, too far in future)
   - ✅ Invalid amounts rejected (unparseable)
   - ✅ Invalid currencies rejected (non-ISO 4217)
   - ✅ Failed validations logged, observation skipped

5. **Idempotency:**
   - ✅ Re-running normalizer produces same canonicals (upsert by canonical_id)
   - ✅ Observations never modified (read-only)

6. **Logging:**
   - ✅ NormalizationLog created with: successes, failures, warnings, execution time
   - ✅ Failed observations include error details
   - ✅ Log persisted before status update

7. **Provenance:**
   - ✅ ProvenanceLedger entry created: `normalize.completed` or `normalize.failed`
   - ✅ Includes: upload_id, normalizer_version, counts

8. **Error handling:**
   - ✅ Partial success allowed (some observations fail, others succeed)
   - ✅ Complete failure → status `error`
   - ✅ All errors captured in NormalizationLog

9. **Performance:**
   - ✅ P95 latency: <10s for 100 observations
   - ✅ P95 latency: <60s for 1000 observations
   - ✅ Process 500 observations/second on standard hardware

10. **Configuration:**
    - ✅ Normalization rules externalized (not hardcoded)
    - ✅ Different rules per source_type (locale, categories, merchants)
    - ✅ Rules versioned (changes tracked in ProvenanceLedger)

---

### 9. Logs y Observability

#### NormalizationLog Structure

**Path:** `/logs/normalize/{upload_id}.log.json`

**Schema:** `docs/schemas/normalization-log.schema.json`

**Example:**
```json
{
  "upload_id": "UL_abc123",
  "normalizer_version": "1.0.0",
  "rule_set_version": "2024.10",
  "started_at": "2025-10-23T14:35:10Z",
  "completed_at": "2025-10-23T14:35:15Z",
  "duration_ms": 5120,
  "observations_processed": 100,
  "canonicals_created": 95,
  "failures": 5,
  "warnings": 12,
  "failure_details": [
    {
      "observation_id": "UL_abc123:42",
      "field": "date",
      "raw_value": "invalid_date",
      "error": "Unparseable date format"
    },
    {
      "observation_id": "UL_abc123:78",
      "field": "currency",
      "raw_value": "US",
      "error": "Invalid ISO 4217 code"
    }
  ],
  "warning_details": [
    {
      "canonical_id": "CT_xyz789",
      "flag": "possible_duplicate",
      "message": "Similar transaction found within 1 day"
    },
    {
      "canonical_id": "CT_abc456",
      "flag": "large_amount",
      "message": "Amount exceeds $10,000"
    }
  ],
  "rules_applied": {
    "date_locale": "en_US",
    "merchant_normalization": true,
    "category_inference": true,
    "duplicate_detection": true
  }
}
```

#### Metrics (Prometheus/StatsD)

```python
# Throughput
normalization.observations.processed.count
normalization.canonicals.created.count
normalization.failures.count

# Latency
normalization.duration.ms (histogram)
normalization.per_observation.ms (histogram)

# Quality
normalization.validation_failures.by_field (counter, labels: field)
normalization.warnings.by_type (counter, labels: warning_type)
normalization.duplicate_rate (gauge)

# Rules
normalization.rule_set.version (gauge, label: version)
normalization.category_coverage (gauge)  # % categorized vs uncategorized
```

#### Logs (Structured JSON)

```json
{
  "timestamp": "2025-10-23T14:35:10Z",
  "level": "INFO",
  "service": "normalization-worker",
  "upload_id": "UL_abc123",
  "event": "normalization.started",
  "observations_count": 100,
  "normalizer_version": "1.0.0"
}

{
  "timestamp": "2025-10-23T14:35:15Z",
  "level": "WARN",
  "service": "normalization-worker",
  "upload_id": "UL_abc123",
  "event": "validation.failed",
  "observation_id": "UL_abc123:42",
  "field": "date",
  "raw_value": "invalid_date",
  "error": "Unparseable date format"
}

{
  "timestamp": "2025-10-23T14:35:15Z",
  "level": "INFO",
  "service": "normalization-worker",
  "upload_id": "UL_abc123",
  "event": "normalization.completed",
  "canonicals_created": 95,
  "failures": 5,
  "duration_ms": 5120
}
```

---

### 10. Riesgos y Mitigaciones

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|-----------|
| **Incorrect date locale assumption** | HIGH - Wrong dates in canonicals | MEDIUM | Emit warnings for ambiguous dates, allow user to specify locale per source |
| **Amount parsing errors** | HIGH - Wrong amounts in canonicals | LOW | Strict validation, reject unparseable amounts, log for manual review |
| **Category inference errors** | MEDIUM - Poor UX, manual recategorization | HIGH | Default to "Uncategorized", allow user to train rules |
| **Duplicate detection false positives** | LOW - User sees duplicate warnings incorrectly | MEDIUM | Flag only, don't block normalization, allow user to confirm/dismiss |
| **Rule changes break re-normalization** | MEDIUM - Can't regenerate canonicals | LOW | Version rule sets, test backwards compatibility |
| **Normalizer crashes on invalid data** | HIGH - Entire upload fails normalization | LOW | Isolate validation per observation, continue on failures |
| **Performance degradation (large files)** | MEDIUM - Slow processing, user frustration | MEDIUM | Batch processing, parallel normalization, optimize validation rules |
| **Merchant normalization inconsistency** | LOW - Same merchant, different canonical names | MEDIUM | Maintain merchant whitelist, fuzzy matching for consistency |

**Key mitigation strategies:**
1. **Fail gracefully:** Allow partial normalization success
2. **Audit trail:** Log all failures and warnings for debugging
3. **User feedback loop:** Let users correct errors, update rules
4. **Idempotency:** Re-normalization safe, original data preserved
5. **Performance:** Batch processing, async workers, indexed queries

---

## Part B: Machinery Layer (Universal Primitives)

### 11. Primitivos Usados

#### 11.1 Normalizer (New Primitive)

**Definition:**
Universal interface for transforming raw observations into validated canonical records.

**Contract:**
```python
class Normalizer(ABC):
    @property
    @abstractmethod
    def version(self) -> str:
        """Semantic version of normalizer (e.g., '1.0.0')"""
        pass

    @abstractmethod
    def normalize(
        self,
        observation: ObservationTransaction,
        rules: NormalizationRuleSet
    ) -> CanonicalTransaction:
        """
        Transform raw observation into validated canonical.

        Raises:
            ValidationError: If observation data is invalid
        """
        pass
```

**Responsibilities:**
- Field transformation (date, amount, description)
- Validation (enforce business rules)
- Enrichment (category, merchant, flags)
- Metadata injection (confidence_score, applied_rules)

**NOT responsible for:**
- Status updates (Coordinator only)
- Persistence (done by Runner)
- Multi-transaction logic (transfers, reconciliation → vertical 1.4)

---

#### 11.2 ValidationEngine (New Primitive)

**Definition:**
Rule-based validation for individual fields.

**Contract:**
```python
class ValidationEngine:
    def validate_date(self, raw: str, locale: str) -> datetime:
        """Parse and validate date"""
        pass

    def validate_amount(self, raw: str) -> Decimal:
        """Parse and validate amount"""
        pass

    def validate_currency(self, code: str) -> str:
        """Validate ISO 4217 currency"""
        pass

    def validate_description(self, raw: str) -> str:
        """Clean and validate description"""
        pass
```

**Reusable across domains:**
- Finance: transaction validation
- Healthcare: lab result validation (date, numeric values)
- Legal: contract date validation
- Research: citation date validation

---

#### 11.3 CanonicalStore (New Primitive)

**Definition:**
Persistent storage for validated canonical records.

**Contract:**
```python
class CanonicalStore:
    def upsert(self, canonical: CanonicalTransaction) -> None:
        """
        Insert or update canonical by canonical_id (idempotent).
        """
        pass

    def get_by_upload(self, upload_id: str) -> list[CanonicalTransaction]:
        """
        Retrieve all canonicals for an upload.
        """
        pass

    def get_by_id(self, canonical_id: str) -> CanonicalTransaction:
        """
        Retrieve single canonical by ID.
        """
        pass

    def query(self, filters: dict) -> list[CanonicalTransaction]:
        """
        Query canonicals by criteria (date range, account, category, etc.)
        """
        pass
```

**Storage properties:**
- Indexed by canonical_id (primary key)
- Indexed by upload_id (for batch retrieval)
- Indexed by (date, account, amount) (for duplicate detection)
- Supports atomic upserts (idempotent re-normalization)

---

#### 11.4 NormalizationRuleSet (New Primitive)

**Definition:**
Configuration for domain-specific normalization rules.

**Contract:**
```python
@dataclass
class NormalizationRuleSet:
    version: str                    # "2024.10"
    date_locale: str                # "en_US", "es_MX", "fr_FR"
    date_formats: list[str]         # ["MM/DD/YYYY", "DD/MM/YYYY"]
    amount_decimal_separator: str   # "." or ","
    merchant_whitelist: dict        # {"STARBUCKS": "Starbucks"}
    category_rules: list[CategoryRule]
    duplicate_tolerance_days: int   # 1 (allow ±1 day for duplicates)
    large_amount_threshold: Decimal # 10000.00
```

**Key principle:**
- Externalized configuration (not hardcoded in normalizer)
- Versioned (changes tracked)
- Per source_type (different rules for BoFA vs Chase)

---

#### 11.5 Existing OL Primitives Used

**UploadRecord:**
- Read: status, observations_count, source_type
- Write: canonicals_count, normalization_log_ref (via Coordinator)

**ObservationStore:**
- Read: raw observations by upload_id
- Never modified (immutable after extraction)

**ProvenanceLedger:**
- Write: normalize.completed or normalize.failed events

**Coordinator:**
- Orchestrate state transitions: parsed → normalizing → normalized

---

### 12. Interlock Patterns

**Pattern 1: Runner NEVER updates status**

```python
# ✅ CORRECT
def run_normalizer(upload_id: str):
    # Runner does work
    result = normalizer.normalize(...)
    canonical_store.upsert(...)

    # Return result to Coordinator
    return NormalizationResult(...)

# ❌ WRONG
def run_normalizer(upload_id: str):
    result = normalizer.normalize(...)
    db.update(UploadRecord, upload_id, status="normalized")  # NEVER!
```

**Enforcement:**
- Code review: Flag any Runner touching `status` field
- Architecture test: Assert Runner cannot import Coordinator
- Database permissions: Runner has read-only access to `status` column

---

**Pattern 2: Validation failures don't stop processing**

```python
# ✅ CORRECT
canonicals = []
failures = []

for obs in observations:
    try:
        canonical = normalizer.normalize(obs, rules)
        canonicals.append(canonical)
    except ValidationError as e:
        failures.append({"observation_id": obs.id, "error": str(e)})
        continue  # Keep processing other observations

# ❌ WRONG
for obs in observations:
    canonical = normalizer.normalize(obs, rules)  # Crash on first error
    canonicals.append(canonical)
```

**Rationale:** Partial success is acceptable (95/100 transactions better than 0/100).

---

**Pattern 3: Observations are immutable**

```python
# ✅ CORRECT
observation = observation_store.get(obs_id)
canonical = normalizer.normalize(observation, rules)  # Read-only
canonical_store.upsert(canonical)  # Write to separate store

# ❌ WRONG
observation = observation_store.get(obs_id)
observation.date = normalize_date(observation.date)  # NEVER mutate!
observation_store.update(observation)
```

**Enforcement:**
- ObservationStore only exposes read methods
- Schema: observations table has no UPDATE trigger
- Architecture: ObservationTransaction is frozen dataclass

---

**Pattern 4: Idempotent re-normalization**

```python
# Normalizer is pure function (same input → same output)
# Re-running produces same canonical_id → upsert is idempotent

# First run
canonical_1 = normalizer.normalize(obs, rules)
canonical_store.upsert(canonical_1)  # INSERT

# Re-run (rules unchanged)
canonical_2 = normalizer.normalize(obs, rules)
canonical_store.upsert(canonical_2)  # UPDATE (same canonical_id)

assert canonical_1.canonical_id == canonical_2.canonical_id
```

**Deterministic canonical_id generation:**
```python
canonical_id = f"CT_{hash(upload_id + row_id + normalizer_version)}"
```

---

### 13. Reusability (Multi-Domain)

**Finance Domain (Current Instantiation):**
- Raw: ObservationTransaction (date, amount, description)
- Canonical: CanonicalTransaction (validated, categorized)
- Rules: US locale, USD currency, merchant normalization

**Healthcare Domain:**
- Raw: ObservationLabResult (test_date, test_name, raw_value, unit)
- Canonical: CanonicalLabResult (validated, normalized units)
- Rules: Date validation, numeric value validation, unit conversion (mg/dL → mmol/L)

**Legal Domain:**
- Raw: ObservationContractClause (clause_text, page_number, extracted_date)
- Canonical: CanonicalClause (validated, categorized by clause type)
- Rules: Date validation, clause taxonomy, risk classification

**Research Domain:**
- Raw: ObservationCitation (raw_citation_text, source_url)
- Canonical: CanonicalCitation (author, title, year, DOI)
- Rules: Citation format parsing (APA, MLA, Chicago), DOI validation

**Manufacturing Domain:**
- Raw: ObservationQCMeasurement (timestamp, sensor_reading, unit)
- Canonical: CanonicalMeasurement (validated, normalized units)
- Rules: Timestamp validation, sensor calibration, unit normalization

**Media Domain:**
- Raw: ObservationTranscript (timestamp, speaker, raw_text)
- Canonical: CanonicalUtterance (validated, speaker_id, cleaned_text)
- Rules: Timestamp validation, speaker normalization, text cleaning

---

**Universal Primitive: Normalizer**

```python
# Finance
class FinanceNormalizer(Normalizer):
    def normalize(self, obs: ObservationTransaction, rules: FinanceRuleSet):
        return CanonicalTransaction(...)

# Healthcare
class HealthcareNormalizer(Normalizer):
    def normalize(self, obs: ObservationLabResult, rules: HealthcareRuleSet):
        return CanonicalLabResult(...)

# Legal
class LegalNormalizer(Normalizer):
    def normalize(self, obs: ObservationContractClause, rules: LegalRuleSet):
        return CanonicalClause(...)
```

**Pattern:** Same interface, different domain models and rule sets.

---

### 14. Machinery Patterns (Implementation-Agnostic)

**Pattern 1: Two-Stage Storage (Raw + Canonical)**

**Separation:**
- ObservationStore: Immutable raw data
- CanonicalStore: Validated, mutable (can be regenerated)

**Benefits:**
- Re-normalization without re-parsing
- Audit trail (compare raw vs canonical)
- Data recovery (if normalization bugs corrupt canonicals)

**Implementation:**
- Separate tables: `observations`, `canonicals`
- Different indexes: observations by upload_id, canonicals by date/account
- Archival: observations can be archived after N days, canonicals retained

---

**Pattern 2: Configurable Validation Rules**

**Externalization:**
- Rules NOT hardcoded in normalizer
- Rules loaded from config (JSON, database, etc.)
- Rules versioned (changes tracked)

**Example:**
```json
{
  "version": "2024.10",
  "source_type": "bofa_pdf",
  "date_locale": "en_US",
  "date_formats": ["MM/DD/YYYY"],
  "merchant_whitelist": {
    "STARBUCKS": "Starbucks",
    "AMAZON.COM": "Amazon"
  },
  "categories": {
    "Starbucks": "Food & Drink",
    "Amazon": "Shopping"
  }
}
```

**Benefits:**
- Update rules without code changes
- A/B test different rule sets
- Per-user customization (if needed)

---

**Pattern 3: Fail-Safe Processing**

**Principle:** One bad observation doesn't block entire upload.

**Implementation:**
```python
for obs in observations:
    try:
        canonical = normalizer.normalize(obs, rules)
        canonical_store.upsert(canonical)
    except ValidationError as e:
        log_failure(obs, e)
        continue  # Keep going
```

**Result:**
- UploadRecord still marked `normalized` if ANY observations succeed
- Failures logged for manual review
- User sees "95/100 transactions" vs "0/100 transactions"

---

**Pattern 4: Metadata-Rich Canonicals**

**Enrichment:** Canonicals include metadata beyond raw data.

**Fields:**
- `confidence_score`: How confident is normalization? (0.0-1.0)
- `applied_rules`: Which rules were applied? (audit trail)
- `flags`: Warnings or notes (`possible_duplicate`, `large_amount`, etc.)

**Benefits:**
- Observability: Why was transaction categorized as X?
- Quality: Filter low-confidence canonicals for manual review
- Learning: Train better rules from flagged transactions

---

### 15. Maturity Checklist

**Production-ready normalization:**

- [x] **Normalizer interface defined** — Universal contract for all domains
- [x] **ValidationEngine primitive** — Field-level validation (date, amount, currency)
- [x] **CanonicalStore primitive** — Persistent storage with idempotent upserts
- [x] **NormalizationRuleSet** — Externalized, versioned configuration
- [x] **Partial success handling** — Don't block entire upload for bad rows
- [x] **Immutable observations** — Raw data never modified
- [x] **Idempotent re-normalization** — Same input → same output
- [x] **Structured logging** — NormalizationLog with failures and warnings
- [x] **Provenance tracking** — ProvenanceLedger entries for all normalizations
- [x] **Runner/Coordinator split enforced** — Runner never touches status
- [ ] **Rule versioning implemented** — Track rule changes in ProvenanceLedger
- [ ] **Performance benchmarks met** — 500 obs/sec on standard hardware
- [ ] **Duplicate detection tested** — False positive rate <5%
- [ ] **Category inference accuracy** — Measured against user corrections
- [ ] **Multi-locale support tested** — US, MX, EU date formats
- [ ] **Error recovery tested** — Re-normalization after rule changes
- [ ] **Load testing completed** — 10k observations/upload, 100 concurrent uploads

---

## Part C: Cross-Cutting Concerns

### 16. Security

**Data integrity:**
- **Validation strictness:** Reject invalid data (unparseable dates, amounts)
- **SQL injection:** Use parameterized queries for all database operations
- **Input sanitization:** Clean descriptions (remove special characters if needed)

**Access control:**
- **Worker permissions:** Read-only access to observations, write to canonicals
- **Coordinator permissions:** Write access to UploadRecord.status
- **Audit:** All normalization events logged in ProvenanceLedger

**Sensitive data handling:**
- **PII in descriptions:** Do NOT log raw descriptions in metrics (may contain merchant details)
- **Amount privacy:** Aggregate metrics OK, individual amounts logged only in secure logs

**Configuration security:**
- **Rule tampering:** NormalizationRuleSet signed/checksummed (detect unauthorized changes)
- **Version control:** All rule changes tracked in git, deployed via CI/CD

---

### 17. Performance

**Latency targets:**
- **P50:** <5s for 100 observations
- **P95:** <10s for 100 observations
- **P99:** <20s for 100 observations
- **P95:** <60s for 1000 observations

**Throughput targets:**
- **Per worker:** 500 observations/second
- **System:** 10,000 observations/second (20 workers)

**Optimization strategies:**

1. **Batch processing:**
   - Fetch all observations for upload in single query
   - Bulk upsert canonicals (transaction batch)

2. **Parallel normalization:**
   - Each observation independent → parallelize with thread pool
   - Example: 100 observations, 10 threads → 10s vs 50s

3. **Index optimization:**
   - ObservationStore: Index on upload_id
   - CanonicalStore: Composite index on (date, account, amount) for duplicate detection

4. **Rule caching:**
   - Load NormalizationRuleSet once, reuse for all observations in upload
   - Cache merchant whitelist in memory (LRU cache)

5. **Lazy validation:**
   - Skip duplicate detection if flag disabled in rules
   - Skip category inference if not needed

**Degradation scenarios:**
- **Large uploads (10k+ observations):** Queue for async processing, show progress bar
- **Rule complexity:** Profile validation rules, optimize slow patterns
- **Database contention:** Retry with exponential backoff

---

### 18. Observability

**Key metrics:**

1. **Throughput:**
   - `normalization.observations.processed.rate` (obs/sec)
   - `normalization.canonicals.created.rate` (canonicals/sec)

2. **Latency:**
   - `normalization.duration.p50` (ms)
   - `normalization.duration.p95` (ms)
   - `normalization.per_observation.p95` (ms)

3. **Quality:**
   - `normalization.failure_rate` (%)
   - `normalization.validation_failures.by_field` (counter, label: field)
   - `normalization.duplicate_rate` (%)
   - `normalization.category_coverage` (% categorized)

4. **Rules:**
   - `normalization.rule_set.version` (gauge, current version)
   - `normalization.rule_changes.count` (counter, rule updates)

**Dashboards:**

**Dashboard 1: Normalization Health**
- Success rate (95%+ target)
- Latency (P50, P95, P99)
- Failure breakdown (by field: date, amount, currency)
- Throughput (observations/sec)

**Dashboard 2: Quality Metrics**
- Category coverage (% uncategorized)
- Duplicate detection rate
- Large amount flags (% transactions >$10k)
- Low confidence transactions (confidence <0.7)

**Dashboard 3: Performance**
- Worker utilization
- Queue depth (uploads waiting for normalization)
- Batch size distribution
- Database query latency

**Alerts:**

```yaml
# Critical
- alert: NormalizationFailureRateHigh
  expr: normalization.failure_rate > 0.10  # >10% failure
  severity: critical

- alert: NormalizationLatencyHigh
  expr: normalization.duration.p95 > 20000  # >20s for 100 obs
  severity: critical

# Warning
- alert: CategoryCoverageLow
  expr: normalization.category_coverage < 0.70  # <70% categorized
  severity: warning

- alert: DuplicateRateHigh
  expr: normalization.duplicate_rate > 0.15  # >15% duplicates
  severity: warning
```

---

### 19. Testing

**Unit tests:**

1. **ValidationEngine:**
   - Test date parsing (all supported formats)
   - Test amount parsing (negatives, parentheses, commas)
   - Test currency validation (valid/invalid ISO codes)
   - Test description cleaning (whitespace, special chars)

2. **Normalizer:**
   - Test field transformations (raw → canonical)
   - Test validation error handling
   - Test confidence scoring
   - Test flag generation (duplicates, large amounts)

3. **CanonicalStore:**
   - Test idempotent upserts (same canonical_id)
   - Test query operations (by upload, by date range)
   - Test duplicate detection

**Integration tests:**

1. **End-to-end normalization:**
   - Upload file → parse → normalize → verify canonicals
   - Test state transitions (parsed → normalizing → normalized)
   - Test partial success (some observations fail)
   - Test complete failure (all observations fail)

2. **Re-normalization:**
   - Normalize observations with rule set v1
   - Update rules to v2
   - Re-normalize same observations
   - Verify canonicals updated correctly

3. **Error scenarios:**
   - Invalid dates → validation failure logged
   - Invalid currency → observation skipped
   - Missing required fields → observation skipped
   - Normalizer crash → coordinator marks error

**Golden data tests:**

```python
# Given: Raw observation (known input)
observation = ObservationTransaction(
    upload_id="UL_test",
    row_id=0,
    date="01/15/2024",
    amount="-5.75",
    description="  STARBUCKS #1234  ",
    currency="USD",
    account="bofa_debit",
    source_type="bofa_pdf",
    parser_id="bofa_pdf_parser",
    parser_version="1.0.0",
    extracted_at="2025-10-23T14:00:00Z"
)

# When: Normalize
rules = NormalizationRuleSet(date_locale="en_US", ...)
canonical = normalizer.normalize(observation, rules)

# Then: Expect canonical (known output)
assert canonical.date == "2025-01-15T00:00:00Z"
assert canonical.amount == Decimal("-5.75")
assert canonical.description == "Starbucks #1234"
assert canonical.merchant == "Starbucks"
assert canonical.category == "Food & Drink"
assert canonical.confidence_score >= 0.9
```

**Performance tests:**

- **Benchmark:** Normalize 100 observations, measure latency
- **Load test:** Normalize 10k observations, verify no memory leaks
- **Concurrency test:** 10 workers normalizing simultaneously

---

### 20. Operations

**Deployment:**

1. **Normalizer versioning:**
   - Deploy new normalizer version with feature flags
   - Canary deployment (5% of uploads use new version)
   - Monitor metrics, rollback if failure rate increases

2. **Rule updates:**
   - Update NormalizationRuleSet in config store
   - Increment version (2024.10 → 2024.11)
   - ProvenanceLedger logs rule version for each normalization

3. **Database migrations:**
   - Add new canonical fields (e.g., `merchant_canonical`)
   - Backfill existing canonicals (re-normalize)
   - No downtime (blue-green deployment)

**Monitoring:**

- **Worker health:** Heartbeat checks, restart on failure
- **Queue depth:** Alert if >100 uploads waiting for normalization
- **Database load:** Monitor query latency, index usage

**Debugging:**

1. **Failed normalization:**
   - Read NormalizationLog: `/logs/normalize/{upload_id}.log.json`
   - Identify failed observation_id
   - Retrieve raw observation from ObservationStore
   - Reproduce locally with same rules

2. **Incorrect canonical:**
   - Compare canonical vs observation
   - Check applied_rules (which rules were used?)
   - Verify rule set version (is it outdated?)
   - Manually re-normalize to verify

3. **Performance issues:**
   - Profile normalizer (which validation is slow?)
   - Check database query logs (slow queries?)
   - Analyze batch sizes (too large?)

**Runbooks:**

**Runbook 1: High failure rate**
1. Check NormalizationLog for common errors
2. If date parsing errors: Verify locale config
3. If currency errors: Check parser output
4. If all errors same field: Investigate validation rule change

**Runbook 2: Slow normalization**
1. Check worker CPU/memory usage
2. Analyze query latency (duplicate detection slow?)
3. Check batch size (split large uploads?)
4. Scale workers horizontally (if queue backed up)

**Runbook 3: Incorrect categories**
1. Retrieve canonical and observation
2. Check merchant extraction (correct merchant name?)
3. Verify category rules (is merchant in whitelist?)
4. Update rules, re-normalize

---

## Summary

**Vertical 1.3 (Normalization) transforms raw observations into validated canonical transactions.**

**Key flows:**
1. Coordinator detects `status="parsed"`
2. Runner loads observations, applies normalizer
3. Normalizer validates and transforms each observation
4. Canonicals persisted to CanonicalStore
5. NormalizationLog written
6. Coordinator updates to `status="normalized"`

**New primitives:**
- **Normalizer** — Universal interface for raw → canonical transformation
- **ValidationEngine** — Field-level validation (date, amount, currency)
- **CanonicalStore** — Persistent storage for validated canonicals
- **NormalizationRuleSet** — Externalized, versioned configuration

**Key patterns:**
- **Fail-safe:** Partial success allowed (don't block for bad rows)
- **Immutable observations:** Raw data never modified
- **Idempotent re-normalization:** Same input → same output
- **Configurable rules:** Not hardcoded, externalized and versioned

**Multi-domain applicability:**
- Finance: transactions
- Healthcare: lab results
- Legal: contract clauses
- Research: citations
- Manufacturing: QC measurements

**Next vertical:** 1.4 (Transfer Linking) — Detect and link paired transactions (debit from account A, credit to account B).
