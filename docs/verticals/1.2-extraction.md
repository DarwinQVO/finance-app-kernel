# Vertical 1.2: Extraction (Parser → Observations)

**Version**: 1.0
**Status**: Formalized
**Last Updated**: 2025-10-23

---

## Part A: Product Layer (Concrete App Behavior)

### 1. Scope

**What this vertical does:**
- Worker detects `UploadRecord` with `status="queued_for_parse"`
- Executes domain-specific parser (e.g., `bofa_pdf_parser`)
- Extracts raw observations (unvalidated transactions)
- Persists to `ObservationStore`
- Writes detailed `ParseLog`
- Coordinator updates status to `parsed` or `error`

**Boundaries:**
- Starts: `UploadRecord.status = "queued_for_parse"`
- Ends: `UploadRecord.status = "parsed"` (or `"error"`)
- Out of scope: Normalization, validation, categorization (vertical 1.3)

---

### 2. Uso Real (User Flow)

**Automated flow (no user interaction):**

1. **Coordinator detects work:**
   - Query: `SELECT upload_id, file_id FROM upload_records WHERE status = 'queued_for_parse' ORDER BY created_at LIMIT 10`
   - Transition: `status → "parsing"`

2. **Runner executes parser:**
   - Retrieve `FileArtifact` by `file_id`
   - Get `storage_ref` → retrieve content from `StorageEngine`
   - Identify parser: `source_type="bofa_pdf"` → `bofa_pdf_parser`
   - Execute parser → `ObservationTransaction[]`

3. **Runner persists observations:**
   - Upsert to `ObservationStore` by `(upload_id, row_id)`
   - Each observation has: `date, description, amount, currency, account, etc.`

4. **Runner writes ParseLog:**
   - Path: `/logs/parse/{upload_id}.log.json`
   - Contains: parser version, execution time, rows extracted, warnings, errors

5. **Coordinator updates status:**
   - Success: `status → "parsed"`, set `observations_count`, `parse_log_ref`
   - Failure: `status → "error"`, set `error_message`, `error_code`

6. **Provenance entry:**
   - Event: `parse.completed` or `parse.failed`
   - Includes: `upload_id`, `parser_id`, `parser_version`, `observations_count`

---

### 3. Primitivos Tocados (OL/RL/IL)

**OL (Objective Layer):**
- `UploadRecord` — State machine (read status, metadata)
- `FileArtifact` — Internal metadata lookup
- `StorageEngine` — Retrieve file content
- `ObservationStore` — Persist raw extracted data
- `ProvenanceLedger` — Record parse events
- `ParserRegistry` — Map source_type → parser_id
- `Parser` (interface) — Base contract for all parsers
- `ParseLog` — Structured execution log
- `Coordinator` — Status orchestration

**RL (Representation Layer):**
- None (backend-only vertical)

**IL (Interface Layer):**
- None (backend-only vertical)

---

### 4. Contracts (API + Internal)

#### Internal Worker Contract

**Query for work:**
```sql
SELECT upload_id, file_id, source_type
FROM upload_records
WHERE status = 'queued_for_parse'
ORDER BY created_at
LIMIT 10;
```

**Coordinator transition (before parsing):**
```python
coordinator.transition(upload_id, to_state="parsing")
```

**Runner execution:**
```python
def run_parser(upload_id: str) -> ParseResult:
    # 1. Get file
    upload_record = db.get(UploadRecord, upload_id)
    file_artifact = db.get(FileArtifact, upload_record.file_id)
    content = storage_engine.retrieve(file_artifact.storage_ref)

    # 2. Get parser
    parser = parser_registry.get_parser(upload_record.source_type)

    # 3. Execute
    observations = parser.parse(content)

    # 4. Persist observations
    for obs in observations:
        observation_store.upsert(obs)

    # 5. Write log
    parse_log = create_parse_log(upload_id, parser, observations)
    write_log(f"/logs/parse/{upload_id}.log.json", parse_log)

    return ParseResult(
        success=True,
        observations_count=len(observations),
        parse_log_ref=f"/logs/parse/{upload_id}.log.json"
    )
```

**Coordinator transition (after parsing):**
```python
# Success
coordinator.transition(
    upload_id,
    to_state="parsed",
    observations_count=42,
    parse_log_ref="/logs/parse/UL_abc123.log.json"
)

# Failure
coordinator.transition(
    upload_id,
    to_state="error",
    error_code="PARSE_FAILED",
    error_message="PDF corrupted at page 3"
)
```

#### Parser Interface Contract

```typescript
interface Parser {
  // Unique identifier (matches ParserRegistry)
  parser_id: string

  // Semantic version
  version: string

  // Parse file content → observations
  parse(content: Bytes): ObservationTransaction[]

  // Validate if parser can handle this file
  can_parse(content: Bytes): boolean
}
```

**Example implementation:**
```python
class BofAPDFParser(Parser):
    parser_id = "bofa_pdf_parser"
    version = "1.2.0"

    def parse(self, content: bytes) -> List[ObservationTransaction]:
        # Extract tables from PDF
        tables = extract_tables_from_pdf(content)

        # Parse each row
        observations = []
        for row_id, row in enumerate(tables[0].rows):
            obs = ObservationTransaction(
                upload_id=self.upload_id,
                row_id=row_id,
                date=row['Date'],
                description=row['Description'],
                amount=row['Amount'],
                currency='USD',  # Hardcoded for BofA
                account='bofa_debit'  # From source_type
            )
            observations.append(obs)

        return observations
```

---

### 5. Schemas / Tipos (persistencia)

#### ObservationTransaction

See [observation-transaction.schema.json](../schemas/observation-transaction.schema.json) for complete schema.

```typescript
interface ObservationTransaction {
  // Composite key (idempotency)
  upload_id: string
  row_id: number  // 0-indexed position in source file

  // Raw extracted fields (unvalidated)
  date: string  // As-is from parser (not yet ISO 8601)
  description: string  // Merchant name, notes, etc.
  amount: string  // As-is from parser (not yet decimal)
  currency: string  // ISO 4217 or parser guess

  // Source metadata
  account: string  // Which account this transaction belongs to
  source_type: string  // e.g., "bofa_pdf"

  // Parsing metadata
  parser_id: string
  parser_version: string
  extracted_at: ISO8601Timestamp

  // Row-level issues (warnings, not errors)
  warnings: string[]  // e.g., ["Date format ambiguous: 01/02/2024"]
}
```

**Database schema:**
```sql
CREATE TABLE observation_transactions (
    upload_id TEXT NOT NULL,
    row_id INTEGER NOT NULL,
    date TEXT NOT NULL,
    description TEXT NOT NULL,
    amount TEXT NOT NULL,
    currency TEXT NOT NULL,
    account TEXT NOT NULL,
    source_type TEXT NOT NULL,
    parser_id TEXT NOT NULL,
    parser_version TEXT NOT NULL,
    extracted_at TIMESTAMPTZ DEFAULT NOW(),
    warnings JSONB DEFAULT '[]',
    PRIMARY KEY (upload_id, row_id),
    FOREIGN KEY (upload_id) REFERENCES upload_records(upload_id)
);

CREATE INDEX idx_observations_upload ON observation_transactions(upload_id);
CREATE INDEX idx_observations_account ON observation_transactions(account);
CREATE INDEX idx_observations_extracted_at ON observation_transactions(extracted_at DESC);
```

#### ParseLog

See [parse-log.schema.json](../schemas/parse-log.schema.json) for complete schema.

```typescript
interface ParseLog {
  // Identity
  upload_id: string

  // Parser info
  parser_id: string
  parser_version: string

  // Execution
  started_at: ISO8601Timestamp
  completed_at: ISO8601Timestamp
  duration_ms: number

  // Results
  success: boolean
  rows_extracted: number
  rows_skipped: number  // e.g., header rows

  // Issues
  warnings: Warning[]
  errors: Error[]

  // File metadata (for debugging)
  file_size_bytes: number
  file_hash: string
  mime_type: string
}

interface Warning {
  row_id?: number  // null if file-level warning
  message: string
  severity: "low" | "medium" | "high"
}

interface Error {
  row_id?: number
  error_code: string  // e.g., "PDF_CORRUPT", "LAYOUT_MISMATCH"
  message: string
  stack_trace?: string
}
```

---

### 6. Validaciones & Estados

#### State Transitions

```
UploadRecord.status transitions:

queued_for_parse → parsing (Coordinator)
    ↓
  parsing → parsed (Coordinator, if success)
    ↓
  parsing → error (Coordinator, if failure)
```

**Validation rules:**

1. **Coordinator can ONLY transition from:**
   - `queued_for_parse → parsing`
   - `parsing → parsed`
   - `parsing → error`

2. **Runner CANNOT:**
   - Update `UploadRecord.status` (violation)
   - Execute parser without Coordinator transition

3. **Parser MUST:**
   - Return at least 0 observations (empty list is valid)
   - Include `row_id` starting at 0
   - Never throw unhandled exceptions (catch → Error in ParseLog)

#### Idempotency Guarantee

**Key:** `(upload_id, row_id)`

```sql
-- Upsert behavior
INSERT INTO observation_transactions (upload_id, row_id, ...)
VALUES ('UL_abc123', 0, ...)
ON CONFLICT (upload_id, row_id)
DO UPDATE SET
    date = EXCLUDED.date,
    description = EXCLUDED.description,
    ...
```

**Why idempotent:**
- Retry logic: If parser crashes mid-execution, re-run is safe
- Same `(upload_id, row_id)` → same observation
- No duplicate rows

---

### 7. Edge Cases

#### Case 1: PDF with 0 transactions

**Scenario:** Bank statement with no transactions (new account, zero activity)

**Behavior:**
- Parser returns `[]` (empty list)
- Coordinator transitions to `parsed`
- `observations_count = 0`
- ParseLog includes warning: `"No transactions found in statement"`
- **NOT an error** — parsed successfully, just empty

**Why not error:**
- Valid business scenario
- Parser executed correctly
- User should see "Upload successful, no transactions"

---

#### Case 2: Corrupted PDF

**Scenario:** File is unreadable (truncated, encrypted, wrong format)

**Behavior:**
- Parser catches exception
- Returns `ParseResult(success=False, error_code="PDF_CORRUPT")`
- Coordinator transitions to `error`
- ParseLog includes error details
- User sees: "Failed to parse: file may be corrupted"

---

#### Case 3: Layout mismatch

**Scenario:** BofA changes PDF format, parser can't find expected tables

**Behavior:**
- Parser detects incompatibility (via `can_parse()` check)
- Returns `ParseResult(success=False, error_code="LAYOUT_MISMATCH")`
- Coordinator transitions to `error`
- ParseLog includes: parser version, expected vs actual layout
- Alert sent to admin: "Parser needs update"

---

#### Case 4: Partial success

**Scenario:** Parser extracts 40 rows successfully, fails on row 41

**Behavior:**
- Parser persists 40 rows to `ObservationStore`
- Returns `ParseResult(success=False, error_code="PARTIAL_PARSE")`
- Coordinator transitions to `error` (not `parsed`)
- ParseLog shows: `rows_extracted=40, errors=[{row_id=41, error_code="AMOUNT_UNPARSABLE"}]`
- User can see 40 rows but knows parsing incomplete

**Why error, not warning:**
- Incomplete data → don't proceed to normalization
- Requires manual intervention or parser fix

---

#### Case 5: Duplicate file re-uploaded

**Scenario:** User uploads same file twice (caught in vertical 1.1)

**Behavior:**
- Vertical 1.1 returns `409 Conflict` with existing `upload_id`
- Vertical 1.2 NEVER runs (dedupe prevents re-parse)
- Observations from first upload remain unchanged

**Note:** This is already handled upstream, but documented here for completeness.

---

#### Case 6: Ambiguous date format

**Scenario:** Date appears as "01/02/2024" (Jan 2 or Feb 1?)

**Behavior:**
- Parser applies heuristic (BofA uses MM/DD/YYYY in USA)
- Extracts as `"01/02/2024"` (raw string, NOT normalized)
- Adds warning: `"Date format ambiguous, assuming MM/DD/YYYY"`
- Normalization (vertical 1.3) will apply locale rules

**Why not error:**
- Parser's job is to extract AS-IS
- Normalization handles interpretation

---

### 8. Acceptance Criteria (Definition of Done)

**Vertical 1.2 is complete when:**

✅ **Core Functionality:**
1. Worker queries `upload_records` where `status='queued_for_parse'`
2. Coordinator transitions to `parsing` before runner starts
3. Runner executes parser, persists observations
4. Runner writes ParseLog with all required fields
5. Coordinator transitions to `parsed` (success) or `error` (failure)
6. ProvenanceLedger has `parse.completed` or `parse.failed` entry

✅ **Idempotency:**
7. Re-running parser on same `upload_id` overwrites observations (safe)
8. `(upload_id, row_id)` uniqueness enforced at DB level

✅ **Edge Cases Handled:**
9. 0 rows → `parsed` with warning (not error)
10. Corrupted PDF → `error` with `PDF_CORRUPT` code
11. Layout mismatch → `error` with `LAYOUT_MISMATCH` code
12. Partial parse → `error` with partial data + error log

✅ **Observability:**
13. ParseLog includes: parser version, duration, row counts, warnings, errors
14. Metrics emitted: `parse_success_rate`, `parse_latency_p95`, `rows_extracted_p50`

✅ **Schema Compliance:**
15. `observation-transaction.schema.json` validates all observations
16. `parse-log.schema.json` validates all logs

✅ **Runner Constraint:**
17. Runner NEVER touches `UploadRecord.status` (only Coordinator)
18. Code review confirms no `upload_record.status = ...` in runner code

---

### 9. Logs & Provenance

#### ParseLog Structure

**Path:** `/logs/parse/{upload_id}.log.json`

**Example (success):**
```json
{
  "upload_id": "UL_abc123",
  "parser_id": "bofa_pdf_parser",
  "parser_version": "1.2.0",
  "started_at": "2025-10-23T14:30:00Z",
  "completed_at": "2025-10-23T14:30:03Z",
  "duration_ms": 3421,
  "success": true,
  "rows_extracted": 42,
  "rows_skipped": 1,
  "warnings": [
    {
      "row_id": 15,
      "message": "Date format ambiguous: 01/02/2024, assuming MM/DD/YYYY",
      "severity": "low"
    }
  ],
  "errors": [],
  "file_size_bytes": 245760,
  "file_hash": "sha256:abc123...",
  "mime_type": "application/pdf"
}
```

**Example (failure):**
```json
{
  "upload_id": "UL_def456",
  "parser_id": "bofa_pdf_parser",
  "parser_version": "1.2.0",
  "started_at": "2025-10-23T15:00:00Z",
  "completed_at": "2025-10-23T15:00:01Z",
  "duration_ms": 1234,
  "success": false,
  "rows_extracted": 0,
  "rows_skipped": 0,
  "warnings": [],
  "errors": [
    {
      "error_code": "PDF_CORRUPT",
      "message": "Failed to open PDF: Invalid header at byte 0",
      "stack_trace": "Traceback (most recent call last):\n  File ..."
    }
  ],
  "file_size_bytes": 1024,
  "file_hash": "sha256:def456...",
  "mime_type": "application/pdf"
}
```

#### ProvenanceLedger Entries

**Event: parse.completed**
```json
{
  "entry_id": "PE_20251023143003_002",
  "timestamp": "2025-10-23T14:30:03Z",
  "upload_id": "UL_abc123",
  "event_type": "parse.completed",
  "actor": {
    "type": "runner",
    "id": "parser-worker-3"
  },
  "data": {
    "parser_id": "bofa_pdf_parser",
    "parser_version": "1.2.0",
    "observations_count": 42,
    "parse_log_ref": "/logs/parse/UL_abc123.log.json",
    "duration_ms": 3421
  },
  "previous_entry_id": "PE_20251023143000_001"
}
```

**Event: parse.failed**
```json
{
  "entry_id": "PE_20251023150001_003",
  "timestamp": "2025-10-23T15:00:01Z",
  "upload_id": "UL_def456",
  "event_type": "parse.failed",
  "actor": {
    "type": "runner",
    "id": "parser-worker-3"
  },
  "data": {
    "parser_id": "bofa_pdf_parser",
    "parser_version": "1.2.0",
    "error_code": "PDF_CORRUPT",
    "error_message": "Failed to open PDF: Invalid header",
    "parse_log_ref": "/logs/parse/UL_def456.log.json"
  },
  "previous_entry_id": "PE_20251023150000_002"
}
```

---

### 10. Riesgos / Diferido

#### Deferred to v2

**Multi-page PDFs with complex layouts:**
- v1: Assume single table per PDF
- v2: Handle multiple tables, cross-page rows

**Parser auto-detection:**
- v1: `source_type` explicitly provided by user
- v2: Detect parser from file content (ML-based classification)

**Incremental parsing:**
- v1: Parse entire file atomically
- v2: Stream large files, parse in chunks

**Parser versioning & migration:**
- v1: Single parser version per source_type
- v2: Side-by-side parser versions, gradual rollout

#### Known Risks

**Risk 1: Parser brittleness**
- **Issue:** Bank changes PDF format, parser breaks
- **Mitigation:**
  - `can_parse()` validation before execution
  - Alert on LAYOUT_MISMATCH errors
  - Version parsers, track success rates per version

**Risk 2: Large files (>50MB)**
- **Issue:** Memory exhaustion, timeout
- **Mitigation:**
  - Max file size enforced in vertical 1.1 (50MB)
  - If needed, increase worker memory or implement streaming

**Risk 3: Slow parsers**
- **Issue:** PDF parsing takes >30s, blocks queue
- **Mitigation:**
  - Timeout per parser (configurable, default 30s)
  - If timeout, transition to `error`, log timeout
  - Consider async workers with longer timeouts

---

## Part B: Machinery Layer (Reusable Factory)

### 11. Primitivas Resultantes

This vertical crystallizes the following **universal multi-domain primitives:**

#### Parser (Interface)

**Type**: Universal extraction contract
**Domain applicability**: ANY document/file processing system

```typescript
interface Parser {
  parser_id: string
  version: string
  parse(content: Bytes): Observation[]
  can_parse(content: Bytes): boolean
}
```

**Multi-domain instantiations:**

**Finance Domain:**
```typescript
class BofAPDFParser implements Parser {
  parse(content: Bytes): ObservationTransaction[]
}
```

**Healthcare Domain:**
```typescript
class LabResultParser implements Parser {
  parse(content: Bytes): ObservationLabResult[]
}
```

**Legal Domain:**
```typescript
class ContractParser implements Parser {
  parse(content: Bytes): ObservationClause[]
}
```

**Key insight:** The `Parser` interface is domain-agnostic. Only the `Observation` type changes per domain.

---

#### ObservationStore

**Type**: Universal raw data storage
**Domain applicability**: ANY extraction pipeline requiring idempotent storage

**Purpose:** Store raw extracted data (unvalidated, as-is from parser) with idempotent upsert semantics.

**Universal pattern:**
```typescript
interface ObservationStore<T> {
  upsert(observation: T): void
  get_by_upload_id(upload_id: string): T[]
  count(upload_id: string): number
}
```

**Multi-domain instantiations:**

**Finance:**
```python
observation_store.upsert(ObservationTransaction)
```

**Healthcare:**
```python
observation_store.upsert(ObservationLabResult)
```

**Research:**
```python
observation_store.upsert(ObservationDataPoint)
```

**Key properties:**
- Idempotent by `(upload_id, row_id)`
- No validation (stores AS-IS)
- Fast batch upserts

---

#### ParseLog

**Type**: Universal execution log
**Domain applicability**: ANY async worker execution requiring observability

**Purpose:** Structured log capturing parser execution details (duration, success/failure, warnings, errors).

**Universal pattern applies to:**
- **Finance:** Parse bank statements
- **Healthcare:** Extract lab results from HL7 messages
- **Legal:** Parse contracts for clauses
- **Data Engineering:** ETL job execution logs
- **Image Processing:** OCR execution logs

---

#### ParserRegistry

**Type**: Universal plugin registry
**Domain applicability**: ANY system with pluggable extractors/processors

**Purpose:** Map source type to parser implementation.

```typescript
interface ParserRegistry {
  register(source_type: string, parser: Parser): void
  get_parser(source_type: string): Parser
  list_parsers(): ParserMetadata[]
}
```

**Multi-domain examples:**

**Finance:**
```python
registry.register("bofa_pdf", BofAPDFParser())
registry.register("chase_csv", ChaseCSVParser())
```

**Healthcare:**
```python
registry.register("hl7_v2", HL7Parser())
registry.register("dicom", DICOMParser())
```

**Legal:**
```python
registry.register("docx_contract", DocxContractParser())
registry.register("pdf_contract", PdfContractParser())
```

---

### 12. Interlocks entre verticales

**Dependencies (what 1.2 requires):**

- **1.1 (Upload Flow):**
  - Provides: `UploadRecord` with `status="queued_for_parse"`, `file_id`
  - Provides: `FileArtifact` with `storage_ref` for content retrieval

**Consumers (what depends on 1.2):**

- **1.3 (Normalization):**
  - Reads: `ObservationTransaction` from `ObservationStore`
  - Triggered by: `UploadRecord.status = "parsed"`

- **2.2 (OL Exploration):**
  - Displays: Raw observations alongside canonicals for comparison
  - Reads: `ParseLog` to show parsing metadata

- **4.3 (Corrections Flow):**
  - References: Original observation when user corrects canonical

- **5.3 (Rule Performance):**
  - Analyzes: ParseLog for parser success rates, latencies

---

### 13. Reusabilidad

This extraction pattern is **universal across ANY multi-stage processing pipeline**:

#### Finance Domain
- Upload bank statement → Parse transactions
- Store raw observations → Normalize to canonical

#### Healthcare Domain
- Upload lab report → Parse test results
- Store raw observations → Normalize units/ranges

#### Legal Domain
- Upload contract → Parse clauses
- Store raw clauses → Normalize dates/parties

#### E-commerce Domain
- Upload product catalog → Parse items
- Store raw items → Normalize categories/prices

#### Research Domain
- Upload experiment data → Parse measurements
- Store raw measurements → Normalize units/timestamps

**Common pattern:**
1. Worker detects work (queued_for_X)
2. Coordinator transitions state (X-ing)
3. Runner executes extractor
4. Runner persists raw data
5. Runner writes execution log
6. Coordinator transitions state (X-ed or error)

---

### 14. Abstracción patrón

This vertical demonstrates the **Extract-Transform-Load (ETL) Extract Phase** pattern:

**Universal components:**
- **Source:** File in storage (PDF, CSV, JSON, etc.)
- **Extractor:** Parser that reads source, emits observations
- **Sink:** ObservationStore (raw data storage)
- **Orchestrator:** Coordinator managing state transitions
- **Worker:** Runner executing extractor
- **Audit:** ParseLog capturing execution details

**Key architectural decisions (universal):**
1. **Separation of concerns:** Runner writes data, Coordinator manages state
2. **Idempotency:** `(upload_id, row_id)` key prevents duplicates on retry
3. **Raw-first:** Store AS-IS, validate later (vertical 1.3)
4. **Structured logging:** ParseLog enables debugging, monitoring
5. **Graceful degradation:** 0 rows is success, partial parse is failure

---

### 15. Métricas de madurez

**Current maturity: Specification complete, ready for implementation**

#### What's defined:
✅ State machine transitions (Coordinator-only)
✅ Parser interface contract
✅ ObservationStore schema with idempotency
✅ ParseLog format
✅ Edge case handling (0 rows, corrupted PDF, layout mismatch)
✅ Runner/Coordinator separation enforced
✅ ProvenanceLedger integration

#### What's missing (implement phase):
⚠️ Actual parser implementations (bofa_pdf_parser, chase_csv_parser, etc.)
⚠️ ParserRegistry implementation
⚠️ Worker queue (job polling, retry logic)
⚠️ Timeout handling (kill parser after 30s)
⚠️ Metrics collection (parse_success_rate, latency)
⚠️ Integration tests (end-to-end upload → parse → observations)

#### Stability assessment:
- **Core concepts:** 🟢 Stable (Parser, ObservationStore, ParseLog)
- **State machine:** 🟢 Stable (queued → parsing → parsed/error)
- **Runner constraint:** 🟢 Stable (no status updates in runner)
- **Schema versioning:** 🟡 Pending (need migration strategy for ObservationTransaction schema changes)
- **Parser versioning:** 🟡 Pending (how to handle parser upgrades, side-by-side versions)

---

## Part C: Cross-Cutting Concerns

### 16. Security

#### Access Control

**Who can trigger parsing:**
- Backend workers (internal service accounts)
- NOT exposed via public API

**Data sensitivity:**
- Observations contain PII (transaction descriptions, amounts)
- ParseLog may contain file_hash (non-sensitive) and error traces (may leak structure)

**Mitigation:**
- ObservationStore: Row-level security (filter by user_id in multi-tenant setup)
- ParseLog: Redact sensitive data in error messages (e.g., don't include account numbers)
- ProvenanceLedger: Access restricted to authenticated users

#### Injection Prevention

**Risk:** Malicious PDF with embedded scripts

**Mitigation:**
- Parser runs in sandboxed environment (container, no network access)
- PDF parsing library with known vulnerabilities patched
- Max file size enforced (50MB in vertical 1.1)

---

### 17. Performance

#### Latency Targets

| Operation | Target (p95) | Notes |
|-----------|--------------|-------|
| Poll for work | < 100ms | DB query with index on `status` |
| Retrieve file | < 500ms | Depends on storage backend (S3 vs local) |
| Parse PDF | < 30s | For files < 10MB |
| Persist observations | < 2s | Batch upsert (42 rows) |
| Write ParseLog | < 100ms | Single file write |
| Coordinator transition | < 200ms | DB update + ProvenanceLedger append |

**Total p95 latency (queued → parsed):** < 35s

#### Throughput

- **Workers:** 5 concurrent workers (configurable)
- **Capacity:** 5 files/minute → 300 files/hour
- **Bottleneck:** PDF parsing (CPU-bound)

**Scaling strategy:**
- Horizontal: Add more workers (stateless)
- Vertical: Increase worker CPU allocation

#### Memory Usage

- **Per worker:** 512MB (PDF parsing library + buffering)
- **Max file size:** 50MB → peak memory ~100MB per parse
- **GC pressure:** Medium (many short-lived objects during parsing)

**Optimization:**
- Stream large PDFs page-by-page (v2)
- Use native PDF libraries (C/C++) instead of Python (if bottleneck)

---

### 18. Observability

#### Metrics (Prometheus format)

```python
# Success rate
parse_operations_total{status="success"} 1234
parse_operations_total{status="error"} 56

# Latency (histogram)
parse_duration_seconds{quantile="0.5"} 2.3
parse_duration_seconds{quantile="0.95"} 8.7
parse_duration_seconds{quantile="0.99"} 28.4

# Throughput
parse_rows_extracted_total 52000
parse_rows_extracted_per_file{quantile="0.5"} 42

# Errors by type
parse_errors_total{error_code="PDF_CORRUPT"} 12
parse_errors_total{error_code="LAYOUT_MISMATCH"} 3
parse_errors_total{error_code="TIMEOUT"} 1

# Queue depth
parse_queue_depth{status="queued_for_parse"} 15
parse_queue_depth{status="parsing"} 3
```

#### Logs (Structured JSON)

**Parser start:**
```json
{
  "timestamp": "2025-10-23T14:30:00Z",
  "level": "INFO",
  "stage": "parse",
  "upload_id": "UL_abc123",
  "parser_id": "bofa_pdf_parser",
  "parser_version": "1.2.0",
  "file_size_bytes": 245760,
  "message": "Starting parser execution"
}
```

**Parser complete:**
```json
{
  "timestamp": "2025-10-23T14:30:03Z",
  "level": "INFO",
  "stage": "parse",
  "upload_id": "UL_abc123",
  "parser_id": "bofa_pdf_parser",
  "rows_extracted": 42,
  "duration_ms": 3421,
  "message": "Parser execution completed"
}
```

**Parser error:**
```json
{
  "timestamp": "2025-10-23T15:00:01Z",
  "level": "ERROR",
  "stage": "parse",
  "upload_id": "UL_def456",
  "parser_id": "bofa_pdf_parser",
  "error_code": "PDF_CORRUPT",
  "error_message": "Failed to open PDF: Invalid header",
  "message": "Parser execution failed"
}
```

#### Alerts

**Critical:**
- Parse error rate > 5% (15min window) → PagerDuty
- Parse queue depth > 100 (5min window) → Slack

**Warning:**
- Parse latency p95 > 60s (15min window) → Slack
- Parser version mismatch detected → Email

#### Dashboards

**Parse Pipeline Health:**
- Parse success rate (24h trend)
- Parse latency (p50, p95, p99)
- Queue depth by status
- Error breakdown by error_code

**Parser Performance:**
- Rows extracted per file (histogram)
- Parse duration by parser_id
- Parser version distribution

---

### 19. Testing

#### Unit Tests

**Parser interface:**
```python
def test_bofa_parser_valid_pdf():
    parser = BofAPDFParser()
    content = load_fixture("bofa_statement_valid.pdf")

    observations = parser.parse(content)

    assert len(observations) == 42
    assert observations[0].date == "01/15/2024"
    assert observations[0].amount == "-50.00"
    assert observations[0].currency == "USD"

def test_bofa_parser_corrupted_pdf():
    parser = BofAPDFParser()
    content = load_fixture("bofa_statement_corrupted.pdf")

    with pytest.raises(PDFCorruptError):
        parser.parse(content)

def test_bofa_parser_zero_transactions():
    parser = BofAPDFParser()
    content = load_fixture("bofa_statement_empty.pdf")

    observations = parser.parse(content)

    assert len(observations) == 0  # Valid, not an error
```

**ObservationStore idempotency:**
```python
def test_observation_store_idempotent_upsert():
    store = ObservationStore()
    obs = ObservationTransaction(upload_id="UL_123", row_id=0, ...)

    store.upsert(obs)
    store.upsert(obs)  # Duplicate

    result = store.get_by_upload_id("UL_123")
    assert len(result) == 1  # Only one row

def test_observation_store_update_on_conflict():
    store = ObservationStore()
    obs_v1 = ObservationTransaction(upload_id="UL_123", row_id=0, amount="-50.00")
    obs_v2 = ObservationTransaction(upload_id="UL_123", row_id=0, amount="-55.00")

    store.upsert(obs_v1)
    store.upsert(obs_v2)

    result = store.get_by_upload_id("UL_123")
    assert result[0].amount == "-55.00"  # Updated
```

#### Integration Tests

**End-to-end upload → parse:**
```python
def test_upload_to_parse_flow():
    # Upload file
    response = client.post("/uploads", files={"file": open("bofa.pdf")}, data={"source_type": "bofa_pdf"})
    upload_id = response.json()["upload_id"]
    assert response.status_code == 201

    # Wait for parsing (or trigger manually in test)
    time.sleep(5)

    # Verify observations
    observations = observation_store.get_by_upload_id(upload_id)
    assert len(observations) > 0

    # Verify status
    upload_record = db.get(UploadRecord, upload_id)
    assert upload_record.status == "parsed"
    assert upload_record.observations_count == len(observations)

    # Verify ParseLog
    parse_log = read_log(f"/logs/parse/{upload_id}.log.json")
    assert parse_log["success"] == True
    assert parse_log["rows_extracted"] == len(observations)
```

#### Golden Data Tests

**Fixtures:**
- `fixtures/bofa_statement_2024-01.pdf` → 42 expected observations
- `fixtures/bofa_statement_empty.pdf` → 0 expected observations
- `fixtures/bofa_statement_corrupted.pdf` → parse error expected

**Regression test:**
```python
def test_bofa_parser_golden_data():
    parser = BofAPDFParser()
    content = load_fixture("bofa_statement_2024-01.pdf")
    expected = load_json("bofa_statement_2024-01.expected.json")

    observations = parser.parse(content)

    assert len(observations) == len(expected)
    for obs, exp in zip(observations, expected):
        assert obs.date == exp["date"]
        assert obs.description == exp["description"]
        assert obs.amount == exp["amount"]
```

---

### 20. Operations

#### Deployment

**Worker deployment:**
- Docker container with parser dependencies
- Environment: `PARSER_TIMEOUT=30`, `MAX_FILE_SIZE_MB=50`
- Health check: `/health` endpoint (liveness: can connect to DB, readiness: queue not full)

**Rolling updates:**
- Deploy new parser version alongside old
- Gradually shift traffic via `parser_version` in ParserRegistry
- Monitor error rates, rollback if needed

#### Monitoring Runbook

**Alert: Parse error rate > 5%**

1. Check dashboard: Which `error_code` is most common?
2. If `PDF_CORRUPT`: Check if specific upload_ids, investigate file source
3. If `LAYOUT_MISMATCH`: Bank may have changed format, update parser
4. If `TIMEOUT`: Increase timeout or add more workers

**Alert: Parse queue depth > 100**

1. Check worker status: Are workers running?
2. Check parser latency: Is parsing slower than usual?
3. Scale workers horizontally (add more pods/instances)
4. If persistent, investigate bottleneck (DB, storage I/O)

#### Retry Logic

**Automatic retries (handled by worker queue):**
- Max retries: 3
- Backoff: Exponential (2s → 4s → 8s)
- Retry on: Transient errors (timeout, network, DB unavailable)
- NO retry on: Permanent errors (PDF_CORRUPT, LAYOUT_MISMATCH)

**Manual retry (via admin UI, v2):**
- Admin marks upload as `queued_for_parse` again
- Coordinator re-triggers parser
- Useful after parser version upgrade

#### Incident Response

**Parser down:**
1. Queue fills up (uploads stuck in `queued_for_parse`)
2. Alert fires (queue depth > 100)
3. Check worker logs for crashes
4. Restart workers, verify health check passes

**Parser producing bad data:**
1. User reports incorrect observations
2. Check ParseLog for warnings
3. Compare observations vs source PDF manually
4. If parser bug: Fix parser, mark affected uploads for re-parse
5. If source data issue: Document in ParseLog warnings

---

**Last Updated**: 2025-10-23
**Maturity**: Spec complete, ready for implementation
**Dependencies**: Vertical 1.1 (Upload Flow)
**Unlocks**: Vertical 1.3 (Normalization)
